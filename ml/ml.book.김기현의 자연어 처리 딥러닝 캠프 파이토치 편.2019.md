## 0장 윈도우 개발 환경 구축
### 0.1 아나콘다 설치
### 0.2 파이토치 설치
### 0.3 깃 설치


## 1장 딥러닝을 활용한 자연어 처리 개요
### 1.1 자연어 처리란 무엇일까?
### 1.2 딥러닝 소개
### 1.3 왜 자연어 처리는 어려울까?
### 1.4 무엇이 한국어 자연어 처리를 더욱 어렵게 만들까?
### 1.5 자연어 처리의 최근 추세


## 2장 기초 수학
### 2.1 확률 변수와 확률 분포
### 2.2 쉬어가기: 몬티 홀 문제
### 2.3 기댓값과 샘플링
### 2.4 MLE
### 2.5 정보 이론
### 2.6 쉬어가기: MSE 손실 함수와 확률 분포 함수
### 2.7 마치며


## 3장 Hello 파이토치
### 3.1 딥러닝을 시작하기 전에
### 3.2 설치 방법
### 3.3 짧은 튜토리얼


## 4장 전처리
### 4.1 전처리
### 4.2 코퍼스 수집
### 4.3 정제
### 4.4 문장 단위 분절
### 4.5 분절
### 4.6 병렬 코퍼스 정렬
### 4.7 서브워드 분절
### 4.8 분절 복원
### 4.9 토치텍스트


## 5장 유사성과 모호성
### 5.1 단어의 의미
### 5.2 원핫 인코딩
### 5.3 시소러스를 활용한 단어 의미 파악
### 5.4 특징
### 5.5 특징 추출하기: TF-IDF
### 5.6 특징 벡터 만들기
### 5.7 벡터 유사도 구하기
### 5.8 단어 중의성 해소
### 5.9 선택 선호도
### 5.10 마치며


## 6장 단어 임베딩
### 6.1 들어가며
### 6.2 차원 축소
### 6.3 흔한 오해 1
### 6.4 word2vec
### 6.5 GloVe
### 6.6 word2vec 예제
### 6.7 마치며


## 7장 시퀀스 모델링
### 7.1 들어가며
### 7.2 순환 신경망
### 7.3 LSTM
### 7.4 GRU
### 7.5 그래디언트 클리핑
### 7.6 마치며


## 8장 텍스트 분류
### 8.1 들어가며
### 8.2 나이브 베이즈 활용하기
### 8.3 흔한 오해 2
### 8.4 RNN 활용하기
### 8.5 CNN 활용하기
### 8.6 쉬어가기: 멀티 레이블 분류
### 8.7 마치며


## 9장 언어 모델링
### 9.1 들어가며
### 9.2 n-gram
### 9.3 언어 모델의 평가 방법
### 9.4 SRILM을 활용하여 n-gram 실습하기
### 9.5 NNLM
### 9.6 언어 모델의 활용
### 9.7 마치며


## 10장 신경망 기계번역
### 10.1 기계번역
### 10.2 seq2seq
### 10.3 어텐션
### 10.4 input feeding
### 10.5 자기회귀 속성과 Teacher forcing 훈련 방법
### 10.6 탐색(추론)
### 10.7 성능 평가
### 10.8 마치며


## 11장 신경망 기계번역 심화 주제
### 11.1 다국어 신경망 번역
### 11.2 단일 언어 코퍼스 활용하기
### 11.3 트랜스포머
### 11.4 마치며


## 12장 강화학습을 활용한 자연어 생성
### 12.1 들어가며
### 12.2 강화학습 기초
### 12.3 정책 기반 강화학습
### 12.4 자연어 생성에 강화학습 적용하기
### 12.5 강화학습을 활용한 지도학습
### 12.6 강화학습을 활용한 비지도학습
### 12.7 마치며


## 13장 듀얼리티 활용
### 13.1 들어가며
### 13.2 듀얼리티를 활용한 지도학습
### 13.3 듀얼리티를 활용한 비지도학습
### 13.4 쉬어가기: Back-translation 재해석하기
### 13.5 마치며


## 14장 NMT 시스템 구축
### 14.1 파이프라인
### 14.2 구글의 NMT
### 14.3 에든버러 대학교의 NMT
### 14.4 MS의 NMT


## 15장 전이학습
### 15.1 전이학습이란
### 15.2 기존의 사전 훈련 방식
### 15.3 ELMo
### 15.4 BERT
### 15.5 OpenAI의 GPT-2
### 15.6 마치며 