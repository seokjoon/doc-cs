## 2장 파이썬 속성 강좌


## 3장 데이터 시각화
### 3.1 matplotlib
### 3.2 막대 그래프
### 3.3 선 그래프
### 3.4 산점도


## 4장 선형대수
### 4.1 벡터
### 4.2 행렬


## 5장 통계
### 5.1 데이터셋 설명하기
### 5.1.1 중심 경향성
### 5.1.2 산포도
### 5.2 상관관계
### 5.3 심슨의 역설
### 5.4 상관관계에 대한 추가적인 경고 사항
### 5.5 상관관계와 인과관계


## 6장 확률
### 6.1 종속성과 독립성
### 6.2 조건부 확률
### 6.3 베이즈 정리
### 6.4 확률변수
### 6.5 연속 분포
### 6.6 정규분포
### 6.7 중심극한정리


## 7장 가설과 추론
### 7.1 통계적 가설검정
### 7.2 예시: 동전 던지기
### 7.3 p-value
### 7.4 신뢰구간
### 7.5 p 해킹
### 7.6 예시: A/B test 해보기
### 7.7 베이즈 추론


## 8장 경사 하강법
### 8.1 경사 하강법에 숨은 의미
### 8.2 그래디언트 계산하기
### 8.3 그래디언트 적용하기
### 8.4 적절한 이동 거리 정하기
### 8.5 경사 하강법으로 모델 학습
### 8.6 미니배치와 SGD(Stochastic Gradient Descent)


## 9장 파이썬으로 데이터 수집하기
### 9.1 stdin과 stdout
### 9.2 파일 읽기
### 9.2.1 텍스트 파일의 기본
### 9.2.2 구분자가 있는 파일
### 9.3 웹 스크래핑
### 9.3.1 HTML과 파싱
### 9.3.2 예시: 의회 감시하기
### 9.4 API 사용하기
### 9.4.1 JSON과 XML
### 9.4.2 인증이 필요하지 않은 API 사용하기
### 9.4.3 API 찾기
### 9.5 예시: 트위터 API 사용하기
### 9.5.1 인증 받기


## 10장 데이터 다루기
### 10.1 데이터 탐색하기
### 10.1.1 1차원 데이터 탐색하기
### 10.1.2 2차원 데이터
### 10.1.3 다차원 데이터
### 10.2 namedtuple 사용하기
### 10.3 Dataclasses
### 10.4 정제하고 합치기
### 10.5 데이터 처리
### 10.6 척도 조절
### 10.7 한편으로: tqdm
### 10.8 차원 축소


## 11장 기계학습
### 11.1 모델링
### 11.2 기계학습이란?
### 11.3 오버피팅과 언더피팅
### 11.4 정확도
### 11.5 Bias-variance 트레이드오프
### 11.6 특성 추출 및 선택


## 12장 k-NN
### 12.1 모델
### 12.2 예시: Iris 데이터
### 12.3 차원의 저주
### 12.4 더 공부해 보고 싶다면


## 13장 나이브 베이즈
### 13.1 바보 스팸 필터
### 13.2 조금 더 똑똑한 스팸 필터
### 13.3 구현하기
### 13.4 모델 검증하기
### 13.5 모델 사용하기


## 14장 단순 회귀 분석
### 14.1 모델
### 14.2 경사 하강법 사용하기
### 14.3 최대가능도추정법


## 15장 다중 회귀 분석
### 15.1 모델
### 15.2 최소자승법에 대한 몇 가지 추가 가정
### 15.3 모델 학습하기
### 15.4 모델 해석하기
### 15.5 적합성(Goodness of fit)
### 15.6 여담: 부트스트랩
### 15.7 계수의 표준 오차
### 15.8 Regularization


## 16장 로지스틱 회귀 분석
### 16.1 문제
### 16.2 로지스틱 함수
### 16.3 모델 적용하기
### 16.4 적합성
### 16.5 서포트 벡터 머신


## 17장 의사결정나무
### 17.1 의사결정나무란?
### 17.2 엔트로피
### 17.3 파티션의 엔트로피
### 17.4 의사결정나무 만들기
### 17.5 종합하기
### 17.6 랜덤포레스트


## 18장 신경망
### 18.1 퍼셉트론
### 18.2 순방향 신경망
### 18.3 역전파
### 18.4 예시: Fizz Buzz


## 19장 딥러닝
### 19.1 텐서
### 19.2 층 추상화
### 19.3 선형 층
### 19.4 순차적인 층으로 구성된 신경망
### 19.5 손실 함수와 최적화
### 19.6 예시: XOR 문제 다시 풀어 보기
### 19.7 다른 활성화 함수
### 19.8 예시: FizzBuzz 다시 풀어 보기
### 19.9 Softmax와 Cross-Entropy
### 19.10 드롭아웃
### 19.11 예시: MNIST
### 19.12 모델 저장 및 불러오기


## 20장 군집화
### 20.1 군집화 감 잡기
### 20.2 모델
### 20.3 예시: 오프라인 모임
### 20.4 k 값 선택하기
### 20.5 예시: 색 군집화하기
### 20.6 상향식 계층 군집화


## 21장 자연어 처리
### 21.1 워드 클라우드
### 21.2 n-그램 언어모델
### 21.3 문법 규칙
### 21.4 여담: 깁스 샘플링
### 21.5 토픽 모델링
### 21.6 단어 벡터
### 21.7 재귀 신경망
### 21.8 예시: 문자 단위의 RNN 사용하기


## 22장 네트워크 분석
### 22.1 매개 중심성
### 22.2 고유벡터 중심성
### 22.2.1 행렬 곱셈
### 22.2.2 중심성
### 22.3 방향성 그래프와 페이지랭크


## 23장 추천 시스템
### 23.1 수작업을 이용한 추천
### 23.2 인기도를 활용한 추천
### 23.3 사용자 기반 협업 필터링
### 23.4 상품 기반 협업 필터링
### 23.5 행렬 분해


## 24장 데이터베이스와 SQL


## 25장 맵리듀스
### 25.1 예시: 단어 수 세기
### 25.2 왜 맵리듀스인가?
### 25.3 맵리듀스 일반화하기
### 25.4 예시: 사용자의 글 분석하기
### 25.5 예시: 행렬 곱셈
### 25.6 여담: Combiner


## 26장 데이터 윤리
### 26.4 나쁜 데이터 제품 만들기
### 26.5 정확도와 공정함의 균형을 유지하기
### 26.6 협력
### 26.7 해석 가능성
### 26.8 추천
### 26.9 편향된 데이터
### 26.10 데이터 보호


## 27장 본격적으로 데이터 과학하기
### 27.1 IPython
### 27.2 수학
### 27.3 밑바닥부터 시작하지 않는 방법
### 27.3.1 NumPy
### 27.3.2 pandas
### 27.3.3 scikit-learn
### 27.3.4 시각화
### 27.3.5 R
### 27.3.6 딥러닝
### 27.4 데이터 찾기
### 27.5 데이터 과학하기