아래 내용 배치

##
* ratsgo/embedding-gpu, nvidia-docker
* 은전한닢 형태소분석
* https://www.cikorea.net/bbs/view/tip?idx=17936

##
* 백오브워즈 가정: td-idf
* 언어모델
* 분포 가정: word2vec

##
* 위키백과 raw 데이터
* 한국어기계독해데이터셋: KorQuAd: korquad.github.io
* 네이버영화리뷰말뭉치: github.com/e9t/nsmc

## 형태소 분석
* KoNLPy: 5개 통합 패키지
* Khaiii: 카카오

## 비지도 형태소 분석
* soynlp: 한국어
* sentencepiece: 구글

## 단어 수준 임베딩
* 예측 기반: NPLM, Word2Vec(구글), FastText(페북)
* 행렬분해 기반: LSA, GloVe, Swivel
* 평가: 단어 유사도, 단어 유추
* 단어 임베딩 시각화: word2vec
* 가중 임베딩

## 문장 수준 임베딩


# ---


## 1장. 서론

### 1.1 임베딩이란
* 가장 간단한 형태의 임베딩: 주제(문서)별 단어의 빈도를 그대로 벡터화


### 1.2 임베딩의 역할
* 1.2.1 단어/문장 간 관련도 계산
	* Word2Vec
		* 단어들을 벡터러 변환: 차원별로
	* 코사인 유사도(cosign similarity), 단어 쌍 간 코사인 유사도
	* t-SNE: 차원 축소(dimension reduction) 기법: 100차원을 2차원으로 줄여 시각화
* 1.2.2 의미/문법 정보 함축
	* 단어 간 덧셈/뺄샘: 단어들 사이의 의미적, 문법적 관계 도출
		* ex: w1 벡터 + w2 벡터 - w3 벡터: 아들 + 딸 - 소녀 = 소년
			* 유추 평가(word analogy)
* 1.2.3 전이 학습
	* 임베딩을 다른 딥러닝 모델의 입력값으로: 전이 학습(transfer learning)
	* 예시: 양방향(bidirectional), LSTM(Long Short-Term Memory), attention
	* FastText 임베딩(100차원)
		* Word2Vec 개선 버전, 59만건 한국어 문서 미리 학습
		* 문장의 극성을 예측
		* 임베딩 품질이 좋으면 수행 태스크 성능(정확도)도 좋음, 모델의 수렴(converge) 빨라짐


### 1.3 임베딩 기법의 역사와 종류
* 1.3.1 통계 기반에서 뉴럴 네트워크 기반으로
	* 초기 임베딩: 말뭉치 통계량을 직접 활용
		* 잠재 의미 분석(Latent Semantic Analysis): 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 행렬(matrix)에 특이값 분해(Singular Value Decomposition) 등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소
			* 행의 개수가 매우 많음: 어휘 수 10만에서 20만개, 행렬 대부분 요소 값이 0(희소 행렬(sparse matrix)
				* 차원을 축소해 사용: 단어를 기준으로(단어 수준 임베딩), 문서를 기준으로(문서 임베딩)
			* 단어-문서 행렬
			* TF-IDF(Term Frequency-Inverse Document Frequency)
			* 단어-문맥 행렬(Word-Context Matrix)
			* 점별 상호 정보량 행렬(Pointwise Mutual Information Matrix)
	* 최근: 뉴럴 네트워크(Neural Network)
		* Neural Probabilistic Language Model, 2023
		* 이전 단어들이 주어질 때 다음 단어 예측, 문장 내 일부분에 구멍을 뚷고(masking) 맞추는 과정에서 학습
* 1.3.2 단어 수준에서 문장 수준으로
	* 단어 수준 모델: 2017년 이전: NPLM, Wo rd2Vec, GloVe, FastText, Swivel
		* 각각 벡터에 해당 단어의 문맥적 의미를 함축
		* 단점: 동음이의어(homonym) 분간 어려움: 단어 형태가 같다면 동일 단어로 보고 모든 문맥 정보를 해당 단어 벡터에 투영
	* 문장 수준 임베딩: ELMo(Embeddings from Language Models): 2018
		* BERT(Bidirectional Encoder Representations from Transformer)
		* GPT(Generative Pre-Training)
		* 단어 시퀀스 전체의 문맥적 의미를 함축: 단어 임베딩보다 전이 학습 효과 좋음
* 1.3.3 룰 → 엔드투엔드 → 프리트레인/파인 튜닝
	* 과거(90년대까지) 사람이 feature(모델의 입력값) 를 직접 뽑음: 언어학적 지식
	* 2000년대 중반 이후: 딥러닝 모델은 입/출격 사이의 관계를 잘 근사(approximation): 데이터 전체를 모델에 넣고 모델 스스로 이해 유도: end-to-end model: sequence-to-sequence 모델
	* 2018 ELMo 모델 이후: 프리트레인(pretrain), 파인 튜닝(fine tuning)
		* 대규모 말뭉치로 임베딩 생성: pretrain: 말뭉치의 의미적, 문법적 맥락 포함됨
			* 이후 임베딩을 입력으로 하는 새 딥러닝 모델 생성
			* 풀고 싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트: 파인 튜닝, 전이 학습
			* ELMo, GPT, BERT 등
	* upstream task: 다운스트림 태스크 이전에 해결할 과제
		* 단어/문장을 pretrain
	* downstream task: 자연어 처리의 구체적 문제들
		* 품사 판별(Part-Of-Speech tagging)
		* 문장 성분 분석
		* 의존 관계 분석
		* 의미역 분석(Semantic Role Labeling)
		* 개체명 인식(Named Entity Recognition)
		* 상호 참조 해결
* 1.3.4 임베딩의 종류와 성능
	* 행렬 분해(factorization) 기반
		* 말뭉치 정보가 들어 있는 행렬을 두개 이상의 작은 행렬로 쪼갬. 둘중 하나만 쓰거나 둘을 더하거나(sum) 이어붙임(concatenate)
		* GloVe, Swivel
	* 예측 기반
		* Word2Vec, FastText, BERT, ELMo, GPT
	* 토픽 기반
		* 문서의 잠재된 주제(latent topic)를 추론(inference)
			* 잠재 디리클레 할당(LDA: Latent Dirichlet Allocation)
				* 각 문서가 어떤 주제 분포(topic distribution)를 갖는지 확률 벡터 형태로 반환
	* 임베딩 성능
		* 문장 임베딩 기법이 단어 임베딩 기법을 크게 앞섬


### 1.4 개발 환경
* 1.4.1 환경 소개
	* ubuntu, py, tensorflow
	* GPU: ELMo, BERT, 단어 임베딩 파인 튜닝
	* nvidia-docker 혹은 aws
* 1.4.2 AWS 구성
* 1.4.3 코드 실행
	* https://github.com/ratsgo/embedding
	* https://ratsgo.github.io/embedding
* 1.4.4 버그 리포트 및 Q&A
* 1.4.5 이 책이 도움받고 있는 오픈소스들


### 1.5 이 책이 다루는 데이터와 주요 용어
* 말뭉치(corpus): 임베딩을 위해 수집한 표본(sample): 말뭉치가 아무리 커도 자연어의 일부만 커버
* 컬렉션(collection): ex: 한국위키백과, 네이버영화리뷰
* 문서(document): 생각/감정/정보를 공유하는 문장 집합, 예시 편의상 줄바꿈(\n)으로 구분된 문자열
	* 단락(paragraph): 문서는 단락의 집합, 예시 편의상 문서와 단락을 구분하지 않음
* 문장(sentence): 편의상 마침표/느낌표/물음표 등 기호로 구분된 문자열
* 토큰(token): 문장은 여러 토큰으로 구성: 단어(word), 형태소(morpheme), 서브워드(subword)
	* 토큰 분리 기준은 케이스별로 다름
	* 토크나이즈(tokenize): 문장을 토큰 시퀀스로 분석
* 어휘 집합(vocabulary)
	* 말뭉치의 모든 문서를 문장으로 나눔 => 토크나이즈 => 중복을 제거한 토큰들의 집합
	* 어휘 집합에 없는 토큰은 미등록 단어(unknown word)



## 2장. 벡터가 어떻게 의미를 가지게 되는가

### 2.1 자연어 계산과 이해
* 임베딩 생성에 사용하는 세 가지 통계 정보
	* 백오브워즈(bag of words)
		* 어떤 단어가 (많이) 쓰였는가
			* 단어의 순서는 무시
		* 대표 통계량: TF-IDF(Term Frequency-Inverse Document Frequency)
		* 대표 모델: Deep Averaging Network
	* 언어모델(language model): 백오브워즈의 대척점
		* 단어가 어떤 순서로 쓰였는가
			* 주어진 단어 시퀀스가 얼마나 자연스러운지 확률 부여: 뉴럴 네트워크 기반 언어 모델
		* 대표 통계량
		* 대표 모델: BLMo, GPT
	* 분포 가능(distribution hypothesis)
		* 어떤 단어가 같이 쓰였는가
			* 단어의 의미는 그 주변 문맥(context)을 통해 유추
		* 대표 통계량: 점별 상호 정보량(PMI: Pointwise Mutual Information)
		* 대표 모델: Word2Vec


### 2.2 어떤 단어가 많이 쓰였는가
* 2.2.1 백오브워즈 가정
	* bag: 중복 원소를 허용한 집합(multiset), 원소 순서는 고려하지 않음
		* 빈도 개수를 사용
			* 경우에 따라서는 등장 여부만 사용
	* 정보 검색(Information Retrieval)에서 여전히 사용
		* 질의를 백오브워즈 임베딩 전환, 질의와 검색 대상 문서 임베딩 간 코사인 유사도, 유사도 높은 순서대로 노출
* 2.2.2 TF-IDF
	* 빈도 방식의 단점: ex: 조사가 많이 등장
		* 단점 보완 위해 TF-IDF
		* 단어-문서 행렬에 가중치 계산으로 행렬 원소(matrix element)를 바꿈
	* TF(Term Frequency): 어떤 단어가 특정 문서에 쓰인 빈도
		* 많이 쓰인 단어가 중요하다는 가정
		* 같은 단어라도 문서마다 다른 값
	* DF(Document Frequency): 특정 단어가 나타난 문서의 수
		* DF가 클수록 다수 문서에 쓰이는 벙용적 단어
		* 문서가 달라지더라도 단어가 같다면 동일값
	* IDF(Inverse Document Frequency): 전체 문서 수(N)를 해당 단어의 DF로 나눈 뒤 로그
		* 값이 클수록 특이한 단어
			* 단어의 주제 예측 능력과 직결: 해당 단어로 문서 주제 가늠
	* 단어의 주제 예측 능력이 강할수록 가중치 커지고 반대의 경우 작아짐
		* 단어의 TF가 높으면 TF-IDF 역시 커짐: 빈도는 주제와 관련 크다는 가정
* 2.2.3 Deep Averaging Network
	* 백오브워즈 가정의 뉴럴 네트워크 버전: 단어의 순서를 고려하지 않음
		* 빈도만 따짐: 해당 문서가 어떤 범주인지 분류(classification)


### 2.3 단어가 어떤 순서로 쓰였는가
* 2.3.1 통계 기반 언어 모델
	* 언어 모델(language model): 단어 시퀀스에 확률(probability) 부여(assign)
		* 백오브워즈의 대척점: 시퀀스(순서)를 무시하느냐 학습하느냐의 차이
	* n개 단어(n-gram)가 동시에 나타날 확률을 반환: 자연스러운 문장에 높은 확률 부여
		* bigram(2-gram), trigram(3-gram), ...
		* 학습한 말뭉치에 등장하지 않는 표현을 확률 0 으로 평가하는 단점
			* 조건부확률(conditional probability), 최대우도추정법(Maximum Likelihood Estimation)
			* n-gram 으로 이 단점 일부 해결
				* 바이그램 해법: 직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사(approximation)
					* 한 상태(state)의 확률은 그 직전 상태에만 의존: 마코프 가정(Markov assumption)
				* n-gram 해법으로 확장: 단어들을 슬라이딩해가며 끝까지 계산
			* 데이터에 한번도 등장하지 않는 n-gram 문제
				* 백오프(back-off): n-gram 등장 빈도를 n 보다 작은 범위의 단어 시퀀스 빈도로 근사
				* 스무딩(smoothing): 등장 빈도 표에 모두 k 만큼 더함
					* 라플라스 스무딩(laplace smoothing): k 가 1
					* 고빈도 문자율 등장 확률 조금 감소, 빈도 없는 문자뎔의 등장 확률 조금 부여
* 2.3.2 뉴럴 네트워크 기반 언어 모델
	* 통계 모델를 뉴럴 네트워크로 학습
	* 주어진 단어 시퀀스로 다음 단어를 맞추는(prediction) 과정에서 학습, 임베딩으로 활용: ELMo, GPT
	* masked language model: 언어 모델과 디테일에서 차이
		* 문장 중간에 마스크 씌우고 예측하는 과정에서 학습
		* 언어 모델 기반 기법은 태생적으로 일방향(uni-directional), 그러나 마스크 언어 모델 기반 기법은 양방향(bi-directional) 가능: 문장 전체를 보고 중간 단어를 예측하므로: 기존 언어 모델 대비 임베딩 품질이 좋음: BERT


### 2.4 어떤 단어가 같이 쓰였는가
* 2.4.1 분포 가정(distributional hypothesis)
	* 분포(distribution): 특정 범위(윈도우) 내에 동시에 등장하는 이웃 단어 또는 문백(context)의 집합
		* 어떤 단어 쌍(pair)이 비슷한 문맥 환경에서 자주 등장한다면 그 의미(meaning) 또한 유사한다는 가정
* 2.4.2 분포와 의미 (1): 형태소(morpheme)
	* 의미를 가지는 최소 단위(더 쪼개면 뜻을 잃음)
* 2.4.3 분포와 의미 (2): 품사
	* 품사 분류 기준: 기능(function), 의미(meaning), 형태(form)
		* 한국어: 체언(명사), 용언(동사/형용사), 관형사, 부사, 조사, 어미, 감탄사
* 2.4.4 점별 상호 정보량(PMI: Pointwise Mutual information)
	* 두 확률변수(random variable) 사이의 상관성을 계량화하는 단위
	* 분포 가정에 따른 단어 가중치 할당 기법: 두 단어가 얼마나 자주 같이 등장하는지
* 2.4.5 Word2Vec



## 3장. 한국어 전처리

### 3.1 데이터 확보
* 3.1.1 한국어 위키백과
* 3.1.2 KorQuAD
* 3.1.3 네이버 영화 리뷰 말뭉치
* 3.1.4 전처리 완료된 데이터 다운로드


### 3.2 지도 학습 기반 형태소 분석
* 3.2.1 KoNLPy 사용법
* 3.2.2 KoNLPy 내 분석기별 성능 차이 분석
* 3.2.3 Khaiii 사용법
* 3.2.4 은전한닢에 사용자 사전 추가하기


### 3.3 비지도 학습 기반 형태소 분석
* 3.3.1 soynlp 형태소 분석기
* 3.3.2 구글 센텐스피스
* 3.3.3 띄어쓰기 교정
* 3.3.4 형태소 분석 완료된 데이터 다운로드



## 4장. 단어 수준 임베딩

### 4.1 NPLM
* 4.1.1 모델 기본 구조
* 4.1.2 NPLM의 학습
* 4.1.3 NPLM과 의미 정보


### 4.2 Word2Vec
* 4.2.1 모델 기본 구조
* 4.2.2 학습 데이터 구축
* 4.2.3 모델 학습
* 4.2.4 튜토리얼


### 4.3 FastText
* 4.3.1 모델 기본 구조
* 4.3.2 튜토리얼
* 4.3.3 한글 자소와 FastText


### 4.4 잠재 의미 분석
* 4.4.1 PPMI 행렬
* 4.4.2 행렬 분해로 이해하는 잠재 의미 분석
* 4.4.3 행렬 분해로 이해하는 Word2Vec
* 4.4.4 튜토리얼


### 4.5 GloVe
* 4.5.1 모델 기본 구조
* 4.5.2 튜토리얼


### 4.6 Swivel
* 4.6.1 모델 기본 구조
* 4.6.2 튜토리얼


### 4.7 어떤 단어 임베딩을 사용할 것인가
* 4.7.1 단어 임베딩 다운로드
* 4.7.2 단어 유사도 평가
* 4.7.3 단어 유추 평가
* 4.7.4 단어 임베딩 시각화


### 4.8 가중 임베딩
* 4.8.1 모델 개요
* 4.8.2 모델 구현
* 4.8.3 튜토리얼



## 5장. 문장 수준 임베딩

### 5.1 잠재 의미 분석


### 5.2 Doc2Vec
* 5.2.1 모델 개요
* 5.2.2 튜토리얼


### 5.3 잠재 디리클레 할당
* 5.3.1 모델 개요
* 5.3.2 아키텍처
* 5.3.3 LDA와 깁스 샘플링
* 5.3.4 튜토리얼


### 5.4 ELMo
* 5.4.1 문자 단위 컨볼루션 레이어
* 5.4.2 양방향 LSTM, 스코어 레이어
* 5.4.3 ELMo 레이어
* 5.4.4 프리트레인 튜토리얼


### 5.5 트랜스포머 네트워크
* 5.5.1 Scaled Dot-Product Attention
* 5.5.2 멀티헤드 어텐션
* 5.5.3 Position-wise Feed-Forward Networks
* 5.5.4 트랜스포머의 학습 전략


### 5.6 BERT
* 5.6.1 BERT, ELMo, GPT
* 5.6.2 프리트레인 태스크와 학습 데이터 구축
* 5.6.3 BERT 모델의 구조
* 5.6.4 프리트레인 튜토리얼



## 6장. 임베딩 파인 튜닝

### 6.1 프리트레인과 파인 튜닝


### 6.2 분류를 위한 파이프라인 만들기


### 6.3 단어 임베딩 활용
* 6.3.1 네트워크 개요
* 6.3.2 네트워크 구현
* 6.3.3 튜토리얼


### 6.4 ELMo 활용
* 6.4.1 네트워크 개요
* 6.4.2 네트워크 구현
* 6.4.3 튜토리얼


### 6.5 BERT 활용
* 6.5.1 네트워크 개요
* 6.5.2 네트워크 구현
* 6.5.3 튜토리얼


### 6.6 어떤 문장 임베딩을 사용할 것인가



## 부록

### 부록 A. 선형대수학 기초
### 1.1 벡터, 행렬 연산
### 1.2 내적과 공분산
### 1.3 내적과 사영
### 1.4 내적과 선형변환
### 1.5 행렬 분해 기반 차원 축소 (1): 주성분 분석(PCA)
### 1.6 행렬 분해 기반 차원 축소 (2): 특이값 분해(SVD)



## 부록 B. 확률론 기초

### 2.1 확률변수와 확률 분포
### 2.2 베이지안 확률론



## 부록 C. 뉴럴 네트워크 기초

### 3.1 DAG로 이해하는 뉴럴 네트워크
### 3.2 뉴럴 네트워크는 확률모델이다
### 3.3 최대우도추정과 학습 손실
### 3.4 그래디언트 디센트
### 3.5 계산 노드별 역전파
### 3.6 CNN과 RNN



## 부록 D. 국어학 기초

### 4.1 통사 단위
### 4.2 문장 유형
### 4.3 품사
### 4.4 상과 시제
### 4.5 주제
### 4.6 높임
### 4.7 양태
### 4.8 의미역
### 4.9 피동
### 4.10 사동
### 4.11 부정
