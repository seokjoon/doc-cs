아래 내용 배치

##
* ratsgo/embedding-gpu, nvidia-docker
* 은전한닢 형태소분석
* https://www.cikorea.net/bbs/view/tip?idx=17936

##
* 백오브워즈 가정: td-idf
* 언어모델
* 분포 가정: word2vec

##
* 위키백과 raw 데이터
* 한국어기계독해데이터셋: KorQuAd: korquad.github.io
* 네이버영화리뷰말뭉치: github.com/e9t/nsmc

## 형태소 분석
* KoNLPy: 5개 통합 패키지
* Khaiii: 카카오

## 비지도 형태소 분석
* soynlp: 한국어
* sentencepiece: 구글

## 단어 수준 임베딩
* 예측 기반: NPLM, Word2Vec(구글), FastTest(페북)
* 행렬분해 기반: LSA, GloVe, Swivel
* 평가: 단어 유사도, 단어 유추
* 단어 임베딩 시각화: word2vec
* 가중 임베딩

## 문장 수준 임베딩


# ---


## 1장. 서론

### 1.1 임베딩이란
* 가장 간단한 형태의 임베딩: 주제(문서)별 단어의 빈도를 그대로 벡터화


### 1.2 임베딩의 역할
* 1.2.1 단어/문장 간 관련도 계산
	* Word2Vec
		* 단어들을 벡터러 변환: 차원별로
	* 코사인 유사도(cosign similarity), 단어 쌍 간 코사인 유사도
	* t-SNE: 차원 축소(dimension reduction) 기법: 100차원을 2차원으로 줄여 시각화
* 1.2.2 의미/문법 정보 함축
	* 단어 간 덧셈/뺄샘: 단어들 사이의 의미적, 문법적 관계 도출
		* ex: w1 벡터 + w2 벡터 - w3 벡터: 아들 + 딸 - 소녀 = 소년
			* 유추 평가(word analogy)
* 1.2.3 전이 학습
	* 임베딩을 다른 딥러닝 모델의 입력값으로: 전이 학습(transfer learning)
	* 예시: 양방향(bidirectional), LSTM(Long Short-Term Memory), attention
	* FastText 임베딩(100차원)
		* Word2Vec 개선 버전, 59만건 한국어 문서 미리 학습
		* 문장의 극성을 예측
		* 임베딩 품질이 좋으면 수행 태스크 성능(정확도)도 좋음, 모델의 수렴(converge) 빨라짐


### 1.3 임베딩 기법의 역사와 종류
* 1.3.1 통계 기반에서 뉴럴 네트워크 기반으로
	* 초기 임베딩: 말뭉치 통계량을 직접 활용
		* 잠재 의미 분석(Latent Semantic Analysis): 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 행렬(matrix)에 특이값 분해(Singular Value Decomposition) 등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소
			* 행의 개수가 매우 많음: 어휘 수 10만에서 20만개, 행렬 대부분 요소 값이 0(희소 행렬(sparse matrix)
				* 차원을 축소해 사용: 단어를 기준으로(단어 수준 임베딩), 문서를 기준으로(문서 임베딩)
			* 단어-문서 행렬
			* TF-IDF(Term Frequency-Inverse Document Frequency)
			* 단어-문맥 행렬(Word-Context Matrix)
			* 점별 상호 정보량 행렬(Pointwise Mutual Information Matrix)
	* 최근: 뉴럴 네트워크(Neural Network)
		* Neural Probabilistic Language Model, 2023
		* 이전 단어들이 주어질 때 다음 단어 예측, 문장 내 일부분에 구멍을 뚷고(masking) 맞추는 과정에서 학습
* 1.3.2 단어 수준에서 문장 수준으로
	* 단어 수준 모델: 2017년 이전: NPLM, Word2Vec, GloVe, FastText, Swivel
		* 각각 벡터에 해당 단어의 문맥적 의미를 함축
		* 단점: 동음이의어(homonym) 분간 어려움: 단어 형태가 같다면 동일 단어로 보고 모든 문맥 정보를 해당 단어 벡터에 투영
	* 문장 수준 임베딩: ELMo(Embeddings from Language Models): 2018
		* BERT(Bidirectional Encoder Representations from Transformer)
* 1.3.3 룰 → 엔드투엔드 → 프리트레인/파인 튜닝
* 1.3.4 임베딩의 종류와 성능


### 1.4 개발 환경
* 1.4.1 환경 소개
* 1.4.2 AWS 구성
* 1.4.3 코드 실행
* 1.4.4 버그 리포트 및 Q&A
* 1.4.5 이 책이 도움받고 있는 오픈소스들


### 1.5 이 책이 다루는 데이터와 주요 용어



## 2장. 벡터가 어떻게 의미를 가지게 되는가

### 2.1 자연어 계산과 이해


### 2.2 어떤 단어가 많이 쓰였는가
* 2.2.1 백오브워즈 가정
* 2.2.2 TF-IDF
* 2.2.3 Deep Averaging Network
### 2.3 단어가 어떤 순서로 쓰였는가
* 2.3.1 통계 기반 언어 모델
* 2.3.2 뉴럴 네트워크 기반 언어 모델
### 2.4 어떤 단어가 같이 쓰였는가
* 2.4.1 분포 가정
* 2.4.2 분포와 의미 (1): 형태소
* 2.4.3 분포와 의미 (2): 품사
* 2.4.4 점별 상호 정보량
* 2.4.5 Word2Vec



## 3장. 한국어 전처리

### 3.1 데이터 확보
* 3.1.1 한국어 위키백과
* 3.1.2 KorQuAD
* 3.1.3 네이버 영화 리뷰 말뭉치
* 3.1.4 전처리 완료된 데이터 다운로드


### 3.2 지도 학습 기반 형태소 분석
* 3.2.1 KoNLPy 사용법
* 3.2.2 KoNLPy 내 분석기별 성능 차이 분석
* 3.2.3 Khaiii 사용법
* 3.2.4 은전한닢에 사용자 사전 추가하기


### 3.3 비지도 학습 기반 형태소 분석
* 3.3.1 soynlp 형태소 분석기
* 3.3.2 구글 센텐스피스
* 3.3.3 띄어쓰기 교정
* 3.3.4 형태소 분석 완료된 데이터 다운로드



## 4장. 단어 수준 임베딩

### 4.1 NPLM
* 4.1.1 모델 기본 구조
* 4.1.2 NPLM의 학습
* 4.1.3 NPLM과 의미 정보


### 4.2 Word2Vec
* 4.2.1 모델 기본 구조
* 4.2.2 학습 데이터 구축
* 4.2.3 모델 학습
* 4.2.4 튜토리얼


### 4.3 FastText
* 4.3.1 모델 기본 구조
* 4.3.2 튜토리얼
* 4.3.3 한글 자소와 FastText


### 4.4 잠재 의미 분석
* 4.4.1 PPMI 행렬
* 4.4.2 행렬 분해로 이해하는 잠재 의미 분석
* 4.4.3 행렬 분해로 이해하는 Word2Vec
* 4.4.4 튜토리얼


### 4.5 GloVe
* 4.5.1 모델 기본 구조
* 4.5.2 튜토리얼


### 4.6 Swivel
* 4.6.1 모델 기본 구조
* 4.6.2 튜토리얼


### 4.7 어떤 단어 임베딩을 사용할 것인가
* 4.7.1 단어 임베딩 다운로드
* 4.7.2 단어 유사도 평가
* 4.7.3 단어 유추 평가
* 4.7.4 단어 임베딩 시각화


### 4.8 가중 임베딩
* 4.8.1 모델 개요
* 4.8.2 모델 구현
* 4.8.3 튜토리얼



## 5장. 문장 수준 임베딩

### 5.1 잠재 의미 분석


### 5.2 Doc2Vec
* 5.2.1 모델 개요
* 5.2.2 튜토리얼


### 5.3 잠재 디리클레 할당
* 5.3.1 모델 개요
* 5.3.2 아키텍처
* 5.3.3 LDA와 깁스 샘플링
* 5.3.4 튜토리얼


### 5.4 ELMo
* 5.4.1 문자 단위 컨볼루션 레이어
* 5.4.2 양방향 LSTM, 스코어 레이어
* 5.4.3 ELMo 레이어
* 5.4.4 프리트레인 튜토리얼


### 5.5 트랜스포머 네트워크
* 5.5.1 Scaled Dot-Product Attention
* 5.5.2 멀티헤드 어텐션
* 5.5.3 Position-wise Feed-Forward Networks
* 5.5.4 트랜스포머의 학습 전략


### 5.6 BERT
* 5.6.1 BERT, ELMo, GPT
* 5.6.2 프리트레인 태스크와 학습 데이터 구축
* 5.6.3 BERT 모델의 구조
* 5.6.4 프리트레인 튜토리얼



## 6장. 임베딩 파인 튜닝

### 6.1 프리트레인과 파인 튜닝


### 6.2 분류를 위한 파이프라인 만들기


### 6.3 단어 임베딩 활용
* 6.3.1 네트워크 개요
* 6.3.2 네트워크 구현
* 6.3.3 튜토리얼


### 6.4 ELMo 활용
* 6.4.1 네트워크 개요
* 6.4.2 네트워크 구현
* 6.4.3 튜토리얼


### 6.5 BERT 활용
* 6.5.1 네트워크 개요
* 6.5.2 네트워크 구현
* 6.5.3 튜토리얼


### 6.6 어떤 문장 임베딩을 사용할 것인가



## 부록

### 부록 A. 선형대수학 기초
### 1.1 벡터, 행렬 연산
### 1.2 내적과 공분산
### 1.3 내적과 사영
### 1.4 내적과 선형변환
### 1.5 행렬 분해 기반 차원 축소 (1): 주성분 분석(PCA)
### 1.6 행렬 분해 기반 차원 축소 (2): 특이값 분해(SVD)



## 부록 B. 확률론 기초

### 2.1 확률변수와 확률 분포
### 2.2 베이지안 확률론



## 부록 C. 뉴럴 네트워크 기초

### 3.1 DAG로 이해하는 뉴럴 네트워크
### 3.2 뉴럴 네트워크는 확률모델이다
### 3.3 최대우도추정과 학습 손실
### 3.4 그래디언트 디센트
### 3.5 계산 노드별 역전파
### 3.6 CNN과 RNN



## 부록 D. 국어학 기초

### 4.1 통사 단위
### 4.2 문장 유형
### 4.3 품사
### 4.4 상과 시제
### 4.5 주제
### 4.6 높임
### 4.7 양태
### 4.8 의미역
### 4.9 피동
### 4.10 사동
### 4.11 부정
