## 1장 트랜스포머란 무엇인가?
* 1.1 트랜스포머 생태계
	* 파운데이션 모델
		* 트랜스포머 모델
			* 단일화: 하나의 모델이 다양한 작업 수행
			* 이머전스: 시스템 행동이 코드가 아닌 데이터로 유추됨
	* 4개 특징: 모델 아키텍처(각 층 병렬 처리시 모든 프로세스에 동일하게 동작), 방대한 데이터, 컴퓨팅 파워, 프롬프트 엔지니어링
* 1.2 트랜스포머로 NLP 모델 최적화
	* 배경
		* 마르코프 프로세스: 무작위 값, 확률적 프로세스: 시퀀스 마지막 요소만으로 다음 요소 예측 가능
		* 규칙 시스템
		* RNN: 연관 신경망: 시퀀스의 지속 상태를 기억
		* CNN: 길고 복잡한 시퀀스에서 장기 의존성 처리시 문제
* 1.3 어떤 리소스를 사용해야 하나요?
	* api
	* 상호참조해결 예시


## 2장 트랜스포머 모델 아키텍처 살펴보기
* 2.1 트랜스포머의 시작: Attention is All You Need
	* 오리지널 모델: 인코더 스택 6개층, 디코더 스택 6개층
		* RNN/LSTM/CNN 전혀 사용하지 않음, 재귀 없음
		* 단어 간 거리 멀수록 재귀(더 많은 파라미터) 대신 어텐션 사용
			* 어텐선: word to word(token to token) 연산
				* 한 단어가 자신을 포함한 시퀀스 내 모든 단어들과 각각 어떻게 연관되는지 계산
					* 단어 벡터 간의 내적(dot product) 사용
	* 인코더 스택
		* 6개 모두 동일 구조, 다른 내용을 담음
		* 입력 임베딩: 입력 토큰을 다차원(d_model=512) 벡터로 변환
			* transduction 모델
			* 토크나이저가 문장을 토큰으로 분리: BPE(byte-pair encoding), word piece, sentence piece
				* 정수 표현 제공
			* 스킵 그램(skip-gram): 주어진 단어를 기초로 context 단어를 예측하도록 학습하는 모델
				* 스탭 크기 2인 윈도우, 한칸씩 움직이며 과정 반복
			* 단어당 512 차원 벡터
			* 코사인 유사도로 단어들의 임베딩 유사 확인
				* 단위구면(unit sphere)에 벡터 표현 위해 유클리드 노름(euclidean norm)
				* cosine_similarity(black, brown) = [[0.9998887]]
			* 위치 인코딩
				* 단어 유사도, 위치 벡터 유사도, 최종적인 위치 인코딩의 유사도
		* 멀티-헤드 어텐션
			* 8개 헤드
			* post-layer normalization



* 2.2 학습과 성능
* 2.3 허깅페이스의 트랜스포머 모델
* 2.4 정리하기
* 2.5 문제
* 2.6 참고 문헌


## 3장 BERT 모델 미세 조정하기
* 3.1 BERT 아키텍처
* 3.2 BERT 미세 조정하기
* 3.3 정리하기
* 3.4 문제
* 3.5 참고 문헌


## 4장 RoBERTa 모델 처음부터 사전 학습하기
* 4.1 토크나이저 학습하기 및 트랜스포머 사전 학습하기
* 4.2 처음부터 KantaiBERT 구축하기
* 4.3 다음 단계
* 4.4 정리하기
* 4.5 문제
* 4.6 참고 문헌


## 5장 RoBERTa 모델 처음부터 사전 학습하기
* 5.1 트랜스포머의 트랜스덕션과 귀납적 상속
* 5.2 트랜스포머 성능 vs 인간 기준값
* 5.3 다운스트림 작업 실행하기
* 5.4 정리하기
* 5.5 문제
* 5.6 참고 문헌


## 6장 트랜스포머를 사용한 기계 번역
* 6.1 기계 번역 정의하기
* 6.2 WMT 데이터셋 전처리하기
* 6.3 BLEU로 기계 번역 평가하기
* 6.4 구글 번역으로 번역하기
* 6.5 트랙스로 번역하기
* 6.6 정리하기
* 6.7 문제
* 6.8 참고 문헌


## 7장 GPT-3 엔진을 사용한 초인간 트랜스포머 등장
* 7.1 GPT-3 트랜스포머 모델을 사용한 초인간 NLP
* 7.2 OpenAI GPT 트랜스포머 모델의 아키텍처
* 7.3 GPT-2를 사용한 일반 텍스트 완성
* 7.4 커스텀 GPT-2 언어 모델 학습
* 7.5 OpenAI GPT-3 작업 실행하기
* 7.6 GPT-2와 GPT-3의 출력 비교하기
* 7.7 GPT-3 미세 조정하기
* 7.8 4차 산업혁명 AI 전문가의 역할
* 7.9 정리하기
* 7.10 문제
* 7.11 참고 문헌


## 8장 법률 및 금융 문서에 트랜스포머를 적용하여 요약하기
* 8.1 범용 텍스트 투 텍스트 모델 디자인하기
* 8.2 T5를 사용해 요약하기
* 8.3 GPT-3로 요약하기
* 8.4 정리하기
* 8.5 문제
* 8.6 참고 문헌


## 9장 데이터셋에 적합한 토크나이저
* 9.1 데이터셋에 적합한 토크나이저
* 9.2 특정 어휘가 포함된 표준 NLP 작업
* 9.3 GPT-3의 범위 살펴보기
* 9.4 정리하기
* 9.5 문제
* 9.6 참고 문헌


## 10장 BERT 기반 트랜스포머를 사용한 SRL
* 10.1 SRL(Semantic Role Labeling, 의미역 결정)
* 10.2 BERT 기반 모델을 사용한 SRL 실험
* 10.3 기본 예시
* 10.4 어려운 예시
* 10.5 SRL 적용 범위에 대한 의문
* 10.6 정리하기
* 10.7 문제
* 10.8 참고 문헌


## 11장 데이터가 말하게 하기: 스토리, 질문, 답변
* 11.1 방법론
* 11.2 방법 0: 시행착오
* 11.3 방법 1: NER
* 11.4 방법 2: SRL
* 11.5 다음 단계
* 11.6 정리하기
* 11.7 문제
* 11.8 참고 문헌


## 12장 고객 감정을 감지해 예측하기
* 12.1 SST(Stanford Sentiment Treebank)
* 12.2 감성 분석으로 고객 행동 예측하기
* 12.3 GPT-3를 사용한 감성 분석
* 12.4 4차 산업 시대에 관한 몇 가지 생각
* 12.5 정리하기
* 12.6 문제
* 12.7 참고 문헌


## 13장 트랜스포머로 가짜 뉴스 분석하기
* 13.1 가짜 뉴스에 대한 감정 반응
* 13.2 가짜 뉴스에 대한 이성적 접근법
* 13.3 마치기 전에
* 13.4 정리하기
* 13.5 문제
* 13.6 참고 문헌


## 14장 블랙박스 트랜스포머 모델 해석하기
* 14.1 BertViz로 트랜스포머 시각화하기
* 14.2 LIT
* 14.3 딕셔너리 러닝을 활용한 트랜스포머 시각화
* 14.4 내부를 볼 수 없는 모델 분석하기
* 14.5 정리하기
* 14.6 문제
* 14.7 참고 문헌


## 15장 NLP부터 범용 트랜스포머 모델까지
* 15.1 모델과 생태계 선택하기
* 15.2 리포머
* 15.3 DeBERTa
* 15.4 범용 모델에서 비전 트랜스포머까지
* 15.5 확장되는 모델 세계
* 15.6 정리하기
* 15.7 문제
* 15.8 참고 문헌


## 16장 트랜스포머 기반 코파일럿의 등장
* 16.1 프롬프트 엔지니어링
* 16.2 코파일럿
* 16.3 도메인별 GPT-3 엔진
* 16.4 컴퓨터 비전
* 16.5 메타버스에서 인간과 AI 코파일럿
* 16.6 정리하기
* 16.7 문제
* 16.8 참고 문헌


## 17장 초인간 트랜스포머를 사용한 OpenAI의 ChatGPT와 GPT-4
* 17.1 ChatGPT와 GPT-4에 초인간 NLP 연동하기
* 17.2 ChatGPT API 시작하기
* 17.3 ChatGPT Plus로 코드와 주석 작성하기
* 17.4 GPT-4 API 시작하기
* 17.5 고급 프롬프트 엔지니어링
* 17.6 설명 가능한 AI(XAI)
* 17.7 DALL-E 2 API 시작하기
* 17.8 모든 것을 종합하기
* 17.9 정리하기
* 17.10 문제
* 17.11 참고 문헌


## 부록 Ⅰ 트랜스포머 용어 설명
* Ⅰ.1 스택
* Ⅰ.2 서브 층
* Ⅰ.3 어텐션 헤드


## 부록 Ⅱ 트랜스포머 모델의 하드웨어 제약사항
* Ⅱ.1 트랜스포머의 아키텍처와 규모
* Ⅱ.2 GPU가 특별한 이유
* Ⅱ.3 GPU는 병렬 연산을 위해 설계되었다
* Ⅱ.4 GPU는 또한 행렬 곱셈을 위해 설계되었다
* Ⅱ.5 GPU를 사용하는 코드
* Ⅱ.6 구글 코랩으로 GPU 테스트하기
* Ⅱ.7 구글 코랩의 무료 CPU
* Ⅱ.8 구글 코랩의 유료 CPU


## 부록 Ⅲ GPT-2를 사용한 일반 텍스트 완성
* Ⅲ.1 1단계: GPU 활성화
* Ⅲ.2 2단계: OpenAI GPT-2 저장소 복제하기
* Ⅲ.3 3단계: 요구사항 설치하기
* Ⅲ.4 4단계: 텐서플로우 버전 확인하기
* Ⅲ.5 5단계: 345M 파라미터 GPT-2 모델 다운로드하기
* Ⅲ.6 6~7단계: 중간 지침
* Ⅲ.7 7b~8단계: 모델 가져오기 및 정의하기
* Ⅲ.8 9단계: GPT-2와 상호 작용하기
* Ⅲ.9 참고 문헌


## 부록 Ⅳ GPT-2를 사용해 커스텀 텍스트 완성하기
* Ⅳ.1 GPT-2 모델 학습하기
* Ⅳ.2 참고 문헌
