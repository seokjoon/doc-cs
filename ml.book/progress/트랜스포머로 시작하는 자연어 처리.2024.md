## 1장 트랜스포머란 무엇인가?
* 1.1 트랜스포머 생태계
	* 파운데이션 모델
		* 트랜스포머 모델
			* 단일화: 하나의 모델이 다양한 작업 수행
			* 이머전스: 시스템 행동이 코드가 아닌 데이터로 유추됨
	* 4개 특징: 모델 아키텍처(각 층 병렬 처리시 모든 프로세스에 동일하게 동작), 방대한 데이터, 컴퓨팅 파워, 프롬프트 엔지니어링
* 1.2 트랜스포머로 NLP 모델 최적화
	* 배경
		* 마르코프 프로세스: 무작위 값, 확률적 프로세스: 시퀀스 마지막 요소만으로 다음 요소 예측 가능
		* 규칙 시스템
		* RNN: 연관 신경망: 시퀀스의 지속 상태를 기억
		* CNN: 길고 복잡한 시퀀스에서 장기 의존성 처리시 문제
* 1.3 어떤 리소스를 사용해야 하나요?
	* api
	* 상호참조해결 예시
* 3.5 참고 문헌


## 2장 트랜스포머 모델 아키텍처 살펴보기
* 2.1 트랜스포머의 시작: Attention is All You Need
	* 오리지널 모델: 인코더 스택 6개층, 디코더 스택 6개층
		* RNN/LSTM/CNN 전혀 사용하지 않음, 재귀 없음
		* 단어 간 거리 멀수록 재귀(더 많은 파라미터) 대신 어텐션 사용
			* 어텐선: word to word(token to token) 연산
				* 한 단어가 자신을 포함한 시퀀스 내 모든 단어들과 각각 어떻게 연관되는지 계산
					* 단어 벡터 간의 내적(dot product) 사용
	* 인코더 스택
		* 6개 모두 동일 구조, 다른 내용을 담음
		* 입력 임베딩: 입력 토큰을 다차원(d_model=512) 벡터로 변환
			* transduction 모델
			* 토크나이저가 문장을 토큰으로 분리: BPE(byte-pair encoding), word piece, sentence piece
				* 정수 표현 제공
			* 스킵 그램(skip-gram): 주어진 단어를 기초로 context 단어를 예측하도록 학습하는 모델
				* 스탭 크기 2인 윈도우, 한칸씩 움직이며 과정 반복
			* 단어당 512 차원 벡터
			* 코사인 유사도로 단어들의 임베딩 유사 확인
				* 단위구면(unit sphere)에 벡터 표현 위해 유클리드 노름(euclidean norm)
				* cosine_similarity(black, brown) = [[0.9998887]]
			* 위치 인코딩
				* 단어 유사도, 위치 벡터 유사도, 최종적인 위치 인코딩의 유사도
		* 멀티-헤드 어텐션
			* 8개 헤드
				* 512 차원을 8개의 64차원으로 나누어 다양한 관점으로 분석: 표현 공간(representation subspace)
				* 출력은 벡터 시퀀스가 아니라 행렬: 연결해서 512차원으로
				* 단어를 3 행렬로 표현, scaled dot-product attention
					* 다른 단어 행렬의 모든 key-value pair 를 탐색하는 64차원 query 행렬
					* 어텐션 점수 구하기 위해 학습한 64차원 key 행렬
					* 다른 어텐션 점수를 구하기 위해 학습한 64차원의 value 행렬
				* 입력 표현 => 가중치 행렬 초기화 => Q, K, V를 얻기 위한 행렬 곱 => 스케일드 어텐션 점수 => 각 벡터의 스케일드 소프트맥스 어텐션 점수 => 어텐션 방정식에 V 대입 => 결과 합산 => 모든 입력에 적용 => 어텐션 서브 층 헤드의 출력 => 각 헤드의 출력 연결 => 포스트-층 정규화
			* post-layer normalization
			* 순방향 네트워크(FFN)
	* 디코더 스택
		* 인코더처럼 6개 층의 구조는 모두 동일
			* multi-head masked attention
			* multi-head attention
			* 완전 연결 위치별 순방향 네트워크
* 2.2 학습과 성능
	* BLEU(Bilingual Evaluation Understudy): 기계번역 결과 품질을 평가
	* 최적화 전략, 기법
* 2.3 허깅페이스의 트랜스포머 모델
	* 허깅스페이스 파이프라인 생성해서 번역
* 2.4 정리하기
	* 자연어 이해(NLU, Natural Language Understanding)
* 3.5 참고 문헌


## 3장 BERT 모델 미세 조정하기
* 3.1 BERT 아키텍처
	* BERT(Bidirectional Encoder Representations form Transformers)
		* 트랜스포머에 multi-head attention sub-layer 추가
		* 트랜스포머의 인코드 블록만 새 방식으로 사용, 디코더 스택은 사용하지 않음
		* 트랜스포머 모델에 양방향 어텐션 적용
	* 인코더 스택
	* 어텐션 헤드가 왼쪽에서 오른쪽, 오른쪽에서 왼쪽으로: 양방향 어텐션
		* MLM(Masked Language Modeling): 마스크드 언어 모델링
			* 단어 분할(subword segmentation) 토큰화: WordPiece
			* 사인-코사인 방식이 아닌 learned positional encoding
		* NSP(Next Sentence Prediction): 다음 문장 예측하기
* 3.2 BERT 미세 조정하기
	* 적합성 판단 다운스트림 작업 예측, 매튜 상관 계수(MCC, Matthews Correlation Coefficient)
	* 허깅페이스 파이토치 인터페이스, 코랩, CUDA
	* 데이터셋 로드, 일반 언어 이해 평가(GLUE, General Language Understanding Evaluation)
	* 문장, 라벨 목록 생성 및 BERT 토큰 추가
	* BERT 토크나이저 활성화
	* 데이터 처리
	* 어텐션 마스크 생성
	* 데이터를 학습 및 검증 셋으로 분할
	* 모든 데이터를 토치 텐서로 변환
	* 배치 크기 선택 및 이터레이터 생성
	* BERT 모델 설정
	* 허깅페이스 BERT 모델 로딩
	* 파라미터 그룹 옵티마이저
	* 학습 루프의 하이퍼파라미터
	* 학습 루프, 학습 평가
	* 홀드아웃 데이터셋을 사용하여 예측 및 평가
	* 매튜 상관 계수를 사용하여 평가
	* 개별 배치의 점수
	* 전체 데이터셋의 매튜 평가
* 3.5 참고 문헌


## 4장 RoBERTa 모델 처음부터 사전 학습하기
* 4.1 토크나이저 학습하기 및 트랜스포머 사전 학습하기
* 4.2 처음부터 KantaiBERT 구축하기
* 4.3 다음 단계


## 5장 트랜스포머를 사용한 다운스트림 NLP 작업
* 5.1 트랜스포머의 트랜스덕션과 귀납적 상속
* 5.2 트랜스포머 성능 vs 인간 기준값
	* SuperGLUE
	* Commitment Bank: 함의(entailment) 필요: 전제(premise), 가설(hypothesis): 중립(neutral), 함의, 반박(contradiction)
	* 다중 문장 독해(MultiRc), 상식 추론 독해 데이터셋(ReCoRD), 텍스트 함의 인식(RTE), 단어 맥락(WiC), 위노그라드 스키마 챌린지(WSC)
* 5.3 다운스트림 작업 실행하기
	* 언어 적합성 말뭉치(CoLA), 스탠퍼드 감성 트리뱅크(SST-2), MS 패러프레이즈 말뭉치(MRPC), 위노그라드 스키마


## 6장 트랜스포머를 사용한 기계 번역
* 6.1 기계 번역 정의하기
	* 인간은 문장을 단어 단위로 번역하지 않음
* 6.2 WMT 데이터셋 전처리하기
* 6.3 BLEU로 기계 번역 평가하기
	* 인간 번역과 기계 번역을 한단어 한단어 비교: Bilingual Evaluation Understudy Score
	* NLTK(Natural Language Toolkit)
* 6.4 구글 번역으로 번역하기
* 6.5 트랙스로 번역하기


## 7장 GPT-3 엔진을 사용한 초인간 트랜스포머 등장
* 7.1 GPT-3 트랜스포머 모델을 사용한 초인간 NLP
* 7.2 OpenAI GPT 트랜스포머 모델의 아키텍처
* 7.3 GPT-2를 사용한 일반 텍스트 완성
* 7.4 커스텀 GPT-2 언어 모델 학습
* 7.5 OpenAI GPT-3 작업 실행하기
	* 프롬프트 변수 예시: 입력값, 설명, 살펴볼 콘텐츠, 예상, 응답
* 7.6 GPT-2와 GPT-3의 출력 비교하기
* 7.7 GPT-3 미세 조정하기


## 8장 법률 및 금융 문서에 트랜스포머를 적용하여 요약하기
* 8.1 범용 텍스트 투 텍스트 모델 디자인하기
	* 단어, 문장, 키워드와 비교(query), 단어의 중요성(values and attention): 트랜스포머의 어텐션 층
	* T5 모델: text-to-text transfer transformer
* 8.2 T5를 사용해 요약하기
* 8.3 GPT-3로 요약하기
	* 프롬프트 P: 설명 E, 텍스트 T, 기대 S
	* Word2Vec
	* 단어가 데이터셋이나 사전에 없는 경우: unknown token
	* 노이즈가 많은 관계, 희귀 단어, 함의


## 9장 데이터셋에 적합한 토크나이저
* 9.1 데이터셋에 적합한 토크나이저
	* 데이터 전처리: 문장부호, 나쁜단어제거, 코드제거, 언어감지, 차별언급제거, 논리검사(자연어추론(NLI)), 부적절정보제거
* 9.2 특정 어휘가 포함된 표준 NLP 작업
* 9.3 GPT-3의 범위 살펴보기


## 10장 BERT 기반 트랜스포머를 사용한 SRL
* 10.1 SRL(Semantic Role Labeling, 의미역 결정)
	* 의미역: 명사 또는 명사구가 문장에서 주동사(main verb)와 관련하여 수행하는 역할
		* 행위자(agent), 주동사/지배동사(governing verb), 서술어(predicate), 수식어
		* 서술어에 대한 명사/명사구는 혼항(arguments) 또는 논항 용어(argument terms)
* 10.2 BERT 기반 모델을 사용한 SRL 실험
* 10.3 기본 예시
* 10.4 어려운 예시
* 10.5 SRL 적용 범위에 대한 의문


## 11장 데이터가 말하게 하기: 스토리, 질문, 답변
* 11.1 방법론
* 11.2 방법 0: 시행착오
* 11.3 방법 1: NER
* 11.4 방법 2: SRL
* 11.5 다음 단계


## 12장 고객 감정을 감지해 예측하기
* 12.1 SST(Stanford Sentiment Treebank)
	* 감성 분석(sentiment analysis)는 합성성의 원리(principle of compositionality)를 따름
* 12.2 감성 분석으로 고객 행동 예측하기
	* 허깅스페이스 DistilBERT
* 12.3 GPT-3를 사용한 감성 분석
* 12.4 4차 산업 시대에 관한 몇 가지 생각


## 13장 트랜스포머로 가짜 뉴스 분석하기
* 13.1 가짜 뉴스에 대한 감정 반응
	* 기울기 시각화(Gradient Visualization)
* 13.2 가짜 뉴스에 대한 이성적 접근법
* 13.3 마치기 전에


## 14장 블랙박스 트랜스포머 모델 해석하기
* 14.1 BertViz로 트랜스포머 시각화하기
* 14.2 LIT
* 14.3 딕셔너리 러닝을 활용한 트랜스포머 시각화
* 14.4 내부를 볼 수 없는 모델 분석하기


## 15장 NLP부터 범용 트랜스포머 모델까지
* 15.1 모델과 생태계 선택하기
* 15.2 리포머
* 15.3 DeBERTa
* 15.4 범용 모델에서 비전 트랜스포머까지
	* 파운데이션 모델의 고유 특성 2가지
		* 이머전스(Emergence): 학습되지 않은 작업 수행 가능. 시퀀스를 이해하는 방법을 학습
		* 단일화(Homogenization): 동일 기본 아키텍처로 여러 도메인에서 사용
	* 비전 트랜스포머
		* Vit: 이미지를 단어 패치로
		* CLIP: 텍스트와 이미지를 인코딩
		* DALL-E: 텍스트로 이미지를 구성
* 15.5 확장되는 모델 세계


## 16장 트랜스포머 기반 코파일럿의 등장
* 16.1 프롬프트 엔지니어링
	* 환유(낱말 대신 그것을 연상시키는 다른 낱말 사용), 생략, 모호한 컨텍스트
* 16.2 코파일럿
* 16.3 도메인별 GPT-3 엔진
	* 트랜스포머 기반 추천 시스템
* 16.4 컴퓨터 비전
* 16.5 메타버스에서 인간과 AI 코파일럿


## 17장 초인간 트랜스포머를 사용한 OpenAI의 ChatGPT와 GPT-4
* 17.1 ChatGPT와 GPT-4에 초인간 NLP 연동하기
* 17.2 ChatGPT API 시작하기
	* ChatCompletion 포맷
	* role: system, assistant, user
* 17.3 ChatGPT Plus로 코드와 주석 작성하기
* 17.4 GPT-4 API 시작하기
* 17.5 고급 프롬프트 엔지니어링
* 17.6 설명 가능한 AI(XAI)
* 17.7 DALL-E 2 API 시작하기
* 17.8 모든 것을 종합하기


## 부록 Ⅰ 트랜스포머 용어 설명
* CNNs(Convolutional Neural Network), RNNs(Recurrent Neural Networks), ANNs(Artificial Neural Networks)
* Ⅰ.1 스택
* Ⅰ.2 서브 층
* Ⅰ.3 어텐션 헤드


## 부록 Ⅱ 트랜스포머 모델의 하드웨어 제약사항
* Ⅱ.1 트랜스포머의 아키텍처와 규모
* Ⅱ.2 GPU가 특별한 이유
* Ⅱ.3 GPU는 병렬 연산을 위해 설계되었다
* Ⅱ.4 GPU는 또한 행렬 곱셈을 위해 설계되었다
* Ⅱ.5 GPU를 사용하는 코드
* Ⅱ.6 구글 코랩으로 GPU 테스트하기
* Ⅱ.7 구글 코랩의 무료 CPU
* Ⅱ.8 구글 코랩의 유료 CPU


## 부록 Ⅲ GPT-2를 사용한 일반 텍스트 완성
* Ⅲ.1 1단계: GPU 활성화
* Ⅲ.2 2단계: OpenAI GPT-2 저장소 복제하기
* Ⅲ.3 3단계: 요구사항 설치하기
* Ⅲ.4 4단계: 텐서플로우 버전 확인하기
* Ⅲ.5 5단계: 345M 파라미터 GPT-2 모델 다운로드하기
* Ⅲ.6 6~7단계: 중간 지침
* Ⅲ.7 7b~8단계: 모델 가져오기 및 정의하기
* Ⅲ.8 9단계: GPT-2와 상호 작용하기
* Ⅲ.9 참고 문헌


## 부록 Ⅳ GPT-2를 사용해 커스텀 텍스트 완성하기
* Ⅳ.1 GPT-2 모델 학습하기
* Ⅳ.2 참고 문헌
