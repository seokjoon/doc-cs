# PART I 검색이 딥러닝을 만나다 1

## CHAPTER 1 신경망을 이용한 검색 3
* 1.1 신경망과 딥러닝 5
* 1.2 머신러닝이란? 8
* 1.3 검색 시에 딥러닝으로 할 수 있는 일은? 10
* 1.4 딥러닝 학습을 위한 계획도 14
* 1.5 유용한 정보 꺼내기 16
	* 수직 검색 엔진(vertical search engine)
	* 검색 엔진: indexing, querying, ranking
* 1.5.1 텍스트, 토큰, 용어, 검색에 관한 기초 지식 18
	* 용어(term), text analysis, keyword search
	* tokenizer: whitespace 만날 때마다 토큰 분할
	* token filter: 불용어(stopword list)의 토큰을 제거
	* inverted index, 역색인 표, query parser
	* 색인 시간 텍스트 분석(index-time text analysis), 찾기 시간 텍스트 분석(search-time text analysis), 어간추출(stemming)
* 1.5.2 연관도 우선 28: relevance
	* 검색 모델(retrieval model): 규칙을 추출할 필요가 없는 모델 정의, 순위(rank) 지정
* 1.5.3 고전적인 검색 모델 29
	* 벡터 공간 모델: VSM: vector space model
		* 용어빈도-역문서빈도: TF-IDF: term frequency-inverse document frequency, 가중치(weight)
			* TF: 단일문서에 용어가 자주 나타날수록 더 중요
			* IDF: 모든 문서에 걸쳐 용어가 흔하게 나타날 경우 덜 중요
* 1.5.4 정밀도와 재현율 30
	* 정밀도(precision), 재현율(recall)
* 1.6 미해결 문제들 31
* 1.7 검색 엔진 블랙박스 열기 32
* 1.8 구조의 손길을 펼치는 딥러닝 34
* 1.9 색인아, 뉴런을 만나 주지 않을래? 38
	* 훈련 후 색인(train-then-index), 색인 후 훈련(index-then-train), 훈련으로 색인 추출(train-extract-index)
* 1.10 신경망 훈련 39
* 1.11 신경 검색의 약속들 42


## CHAPTER 2 동의어 생성 44
	* 질의 결과 개수를 늘리는 일반적 기법: 동의어(synonyms)
* 2.1 동의어 확장 소개 45
* 2.1.1 왜 동의어인가? 47
* 2.1.2 어휘 기반 동의어 일치 49
* 2.2 맥락의 중요성 60
* 2.3 순방향 신경망 62
* 2.4 word2vec 사용 66
* 2.4.1 Deeplearning4j에 word2vec 끼워 쓰기 76
* 2.4.2 Word2vec 기반 동의어 확장 77
* 2.5 평가 및 비교 80
* 2.6 프로덕션 시스템에 대해 고려할 사항 81
* 2.6.1 동의어 대 반의어 83


# PART 2 검색 엔진에 신경망들 던져 넣기 87

## CHAPTER 3 일반 검색에서 텍스트 생성까지 89
* 3.1 정보 요구 대 쿼리: 틈새를 메우는 것 91
* 3.1.1 대안 쿼리 생성 91
* 3.1.2 데이터 준비 94
* 3.1.3 데이터 생성 준비 102
* 3.2 시퀀스 학습 103
* 3.3 재귀 신경망 104
* 3.3.1 RNN 내부 구조와 작동 방식 107
* 3.3.2 장기 의존성 111
* 3.3.3 장단기 기억망 112
* 3.4 비지도 학습 방식으로 텍스트를 생성하기 위한 LSTM 망 113
* 3.4.1 비지도 쿼리 확장 122
* 3.5 비지도 텍스트 생성에서 지도 텍스트 생성까지 126
* 3.5.1 시퀀스-투-시퀀스 모델링 126
* 3.6 프로덕션 시스템에 대해 고려해야 할 점 130


## CHAPTER 4 그럴듯한 쿼리들 제안하기 133
* 4.1 쿼리 제안 생성 134
* 4.1.1 쿼리 작성 중에 제안하기 135
* 4.1.2 사전 기반 제안 136
* 4.2 루씬 룩업 API 136
* 4.3 분석된 내용을 활용하는 제안기 141
* 4.4 언어 모델 사용 148
* 4.5 내용 기반 제안기 152
* 4.6 신경 언어 모델 154
* 4.7 제안용 문자 기반 신경 언어 모델 156
* 4.8 LSTM 언어 모델 조율 160
* 4.9 단어 매장을 이용한 제안 다양화 169


## CHAPTER 5 단어 매장을 사용해 검색 결과의 순위지정하기 173
* 5.1 순위지정의 중요성 174
* 5.2 검색 모델 177
* 5.2.1 TF-IDF와 벡터 공간 모델 179
* 5.2.2 루씬에서 문서의 순위지정하기 183
* 5.2.3 확률 모델 186
* 5.3 신경 정보 검색 188
* 5.4 단어 벡터에서 문서 벡터까지 189
* 5.5 평가 및 비교 196
* 5.5.1 평균 단어 매장 기준 유사도 198


## CHAPTER 6 순위지정 및 추천을 위한 문서 매장 203
* 6.1 단어 매장으로부터 문서 매장까지 204
* 6.2 순위지정 시 단락 벡터 사용 208
* 6.2.1 단락 벡터 기반 유사도 211
* 6.3 문서 매장과 연관 내용 211
* 6.3.1 검색, 추천 그리고 연관 내용 212
* 6.3.2 빈출 용어들을 사용해 유사한 내용 찾기 214
* 6.3.3 단락 벡터를 사용해 유사한 내용 검색 224
* 6.3.4 인코더-디코더 모델에서 벡터를 사용해 유사한 내용 검색 227


# PART 3 한 걸음 더 나아가다 231

## CHAPTER 7 여러 언어로 검색하기 233
* 7.1 언어가 서로 다른 사용자들에게 서비스하기 234
* 7.1.1 문서 번역 대 쿼리 번역 235
* 7.1.2 교차 언어 검색 237
* 7.1.3 루씬 기반 다중 언어 쿼리 239
* 7.2 통계적 기계 번역 241
* 7.2.1 정렬 244
* 7.2.2 단락 기반 번역 245
* 7.3 병렬 말뭉치를 가지고 일하기 246
* 7.4 신경 기계 번역 249
* 7.4.1 인코더-디코더 모델 250
* 7.4.2 DL4J에서 기계 번역을 하기 위한 인코더-디코더 254
* 7.5 여러 언어를 위한 단어 매장 및 문서 매장 261
* 7.5.1 선형 사영 1개 국어 사용 매장 261


## CHAPTER 8 내용 기반 이미지 검색 268
* 8.1 이미지 내용과 검색 270
* 8.2 되돌아보기: 텍스트 기반 이미지 검색 272
* 8.3 이미지 이해하기 275
* 8.3.1 이미지 표현 277
* 8.3.2 특징 추출 280
* 8.4 이미지 표현을 위한 딥러닝 288
* 8.4.1 CNN 290
* 8.4.2 이미지 검색 298
* 8.4.3 국소성 민감 해싱 304
* 8.5 레이블이 없는 이미지 다루기 308


## CHAPTER 9 성능 엿보기 314
* 9.1 성과 및 딥러닝의 약속 315
* 9.1.1 모델 설계로부터 모델 산출로 316
* 9.2 색인과 뉴런이 협동하게 하기 334
* 9.3 데이터 스트림 작업 337
