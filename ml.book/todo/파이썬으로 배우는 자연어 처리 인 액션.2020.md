# PART I 말 많은 컴퓨터: NLP의 기초 1

## CHAPTER 1 사고의 단위: NLP의 개요 3
### 1.1 자연어 대 프로그래밍 언어 4
### 1.2 마법 5
* 1.2.1 대화하는 기계 6
* 1.2.2 수학 7
### 1.3 실제 응용들 9
### 1.4 컴퓨터의 ‘눈’으로 본 언어 11
* 1.4.1 자물쇠 언어 12
* 1.4.2 정규 표현식 13
* 1.4.3 간단한 챗봇 14
* 1.4.4 또 다른 방법 19
### 1.5 짧은 초공간 탐험 23
### 1.6 단어의 순서와 문법 25
### 1.7 챗봇의 자연어 처리 파이프라인 27
### 1.8 더 깊은 처리 30
### 1.9 자연어 IQ 32


## CHAPTER 2 나만의 어휘 구축: 단어 토큰화 37
### 2.1 어려운 문제: 어간 추출의 개요 39
### 2.2 토큰 생성기를 이용한 어휘 구축 40
* 2.2.1 내적 50
* 2.2.2 두 단어 모음의 중복 측정 51
* 2.2.3 토큰 개선 52
* 2.2.4 n-그램을 이용한 어휘 확장 58
* 2.2.5 어휘 정규화 66
### 2.3 감정 분석 76
* 2.3.1 VADER-규칙 기반 감정 분석기 78
* 2.3.2 단순 베이즈 모형 80


## CHAPTER 3 말 잘하는 수학: TF-IDF 벡터 85
### 3.1 단어 모음 86
### 3.2 벡터화 92
* 3.2.1 벡터 공간 95
### 3.3 지프의 법칙 101
### 3.4 주제 모형화 104
* 3.4.1 돌아온 지프 108
* 3.4.2 관련성 순위 110
* 3.4.3 주요 도구: scikit-learn 112
* 3.4.4 여러 TF-IDF 정규화 방법 113
* 3.4.5 Okapi BM25 115
* 3.4.6 다음 단계 116


## CHAPTER 4 단어 빈도에서 의미 찾기: 의미 분석 117
### 4.1 단어 빈도에서 주제 점수로 119
* 4.1.1 TF-IDF 벡터와 표제어 추출 119
* 4.1.2 주제 벡터 120
* 4.1.3 사고 실험 122
* 4.1.4 주제 점수를 매기는 알고리즘 127
* 4.1.5 LDA 분류기 129
### 4.2 잠재 의미 분석(LSA) 134
* 4.2.1 사고 실험의 실현 137
### 4.3 특잇값 분해 140
* 4.3.1 왼쪽 특이 벡터 행렬 U 142
* 4.3.2 특잇값 행렬 S 143
* 4.3.3 오른쪽 특이 벡터 행렬 VT 145
* 4.3.4 SVD 행렬의 방향 145
* 4.3.5 주제 절단 146
### 4.4 주성분 분석(PCA) 148
* 4.4.1 3차원 벡터에 대한 PCA 150
* 4.4.2 말을 떠나 다시 NLP로 돌아가서 152
* 4.4.3 PCA를 이용한 문자 메시지 잠재 의미 분석 154
* 4.4.4 절단된 SVD를 이용한 문자 메시지 잠재 의미 분석 157
* 4.4.5 스팸 분류에 대한 LSA의 정확도 158
### 4.5 잠재 디리클레 할당(LDiA) 161
* 4.5.1 LDiA의 기초 162
* 4.5.2 문자 메시지 말뭉치에 대한 LDiA 주제 모형 165
* 4.5.3 LDiA + LDA = 스팸 분류기 168
* 4.5.4 좀 더 공정한 비교: 주제가 32개인 LDiA 171
### 4.6 거리와 유사도 173
### 4.7 피드백에 기초한 방향 조정 176
* 4.7.1 선형 판별 분석(LDA) 177
### 4.8 주제 벡터의 위력 179
* 4.8.1 의미 기반 검색 181
* 4.8.2 개선안 184


# PART II 더 깊은 학습: 신경망 적용 185

### CHAPTER 5 신경망 첫걸음: 퍼셉트론과 역전파 187
### 5.1 신경망의 구성요소 188
* 5.1.1 퍼셉트론 189
* 5.1.2 디지털 퍼셉트론 190
* 5.1.3 치우침 단위 191
* 5.1.4 오차 곡면을 누비며 207
* 5.1.5 경사로를 따라 활강 208
* 5.1.6 흔들어서 탈출 210
* 5.1.7 케라스: 신경망 파이썬 구현 211
* 5.1.8 더 깊게 배우고 싶다면 215
* 5.1.9 정규화: 스타일 있는 입력 215


## CHAPTER 6 단어 벡터를 이용한 추론: word2vec 활용 217
### 6.1 의미 기반 질의와 비유 218
* 6.1.1 비유 질문 219
### 6.2 단어 벡터 221
* 6.2.1 벡터 지향적 추론 225
* 6.2.2 word2vec의 단어 표현 계산 228
* 6.2.3 gensim.word2vec 모듈 사용법 238
* 6.2.4 나만의 단어 벡터 모형 만들기 241
* 6.2.5 word2vec 대 GloVe 244
* 6.2.6 fastText 245
* 6.2.7 word2vec 대 LSA 246
* 6.2.8 단어 관계의 시각화 247
* 6.2.9 인위적인 단어들 254
* 6.2.10 doc2vec을 이용한 문서 유사도 추정 256


## CHAPTER 7 단어 순서를 고려한 의미 분석: 합성곱 신경망 259
### 7.1 의미의 학습 261
### 7.2 도구 모음 262
### 7.3 합성곱 신경망 264
* 7.3.1 합성곱 신경망의 구조 264
* 7.3.2 단계 크기(보폭) 266
* 7.3.3 필터의 구성 266
* 7.3.4 여백 채우기 268
* 7.3.5 훈련(학습) 270
### 7.4 다시 텍스트로 271
* 7.4.1 케라스로 합성곱 신경망 구현: 자료 준비 273
* 7.4.2 합성곱 신경망의 구조 279
* 7.4.3 풀링 280
* 7.4.4 드롭아웃 283
* 7.4.5 마지막 층 추가 284
* 7.4.6 모형의 저장 및 시험 286
* 7.4.7 모형을 NLP 파이프라인에 도입 289
* 7.4.8 나머지 이야기 290


## CHAPTER 8 돌고 도는 신경망: 순환 신경망 293
### 8.1 과거를 아는 순환 신경망 296
* 8.1.1 시간에 대한 역전파 301
* 8.1.2 무엇을 언제 갱신하는가? 303
* 8.1.3 정리 306
* 8.1.4 항상 그렇듯이 함정이 있다 307
* 8.1.5 케라스를 이용한 순환 신경망 구현 307
### 8.2 모형의 컴파일 312
### 8.3 모형의 훈련 315
### 8.4 초매개변수 조율 316
### 8.5 예측 319
* 8.5.1 상태 유지 320
* 8.5.2 양방향 처리 321
* 8.5.3 순환층 출력의 의미 323


### CHAPTER 9 장단기 기억망(LSTM 망)을 이용한 기억 유지 개선 325
### 9.1 장단기 기억망(LSTM 망) 327
* 9.1.1 시간에 대한 역전파 336
* 9.1.2 예제 문장으로 모형을 시험 339
* 9.1.3 더러운 자료 340
* 9.1.4 다시 더러운 자료로 돌아가서 344
* 9.1.5 단어보다 글자가 쉽다 345
* 9.1.6 말문이 열린 신경망 352
* 9.1.7 구체적인 예제 하나 354
* 9.1.8 무엇을 말할 것인가? 363
* 9.1.9 다른 종류의 기억 수단 363
* 9.1.10 더 깊이 들어가서 364


## CHAPTER 10 순차열 대 순차열 모형과 주의 메커니즘 367
### 10.1 부호기-복호기 구조 368
* 10.1.1 생각 벡터의 복호화 369
* 10.1.2 비슷한 구조들 371
* 10.1.3 대화 생성을 위한 순차열 대 순차열 모형 373
* 10.1.4 LSTM 복습 374
### 10.2 순차열 대 순차열 NLP 파이프라인 구축 375
* 10.2.1 순차열 대 순차열 훈련을 위한 자료 집합 준비 375
* 10.2.2 케라스의 순차열 대 순차열 모형 376
* 10.2.3 순차열 부호기 377
* 10.2.4 생각 벡터 복호기 379
* 10.2.5 순차열 대 순차열 신경망 조립 380
### 10.3 순차열 대 순차열 신경망의 훈련 381
* 10.3.1 출력 순차열 생성 381
### 10.4 순차열 대 순차열 신경망을 이용한 챗봇 구축 383
* 10.4.1 훈련 자료 준비 383
* 10.4.2 문자 사전 구축 384
* 10.4.3 원핫 부호화 훈련 집합 생성 385
* 10.4.4 순차열 대 순차열 챗봇의 훈련 386
* 10.4.5 순차열 생성을 위한 모형 설정 387
* 10.4.6 순차열 생성(예측) 387
* 10.4.7 응답문 생성 및 출력 388
* 10.4.8 챗봇과 대화 389
### 10.5 개선안 390
* 10.5.1 버키팅을 이용한 학습 복잡도 감소 390
* 10.5.2 주의 메커니즘 391
### 10.6 순차열 대 순차열 신경망의 실제 용도 393


# PART III 응용: 실제 NLP 문제들 397

### CHAPTER 11 정보 추출: 개체명 인식과 질의응답 399
### 11.1 개체명과 개체 관계 399
* 11.1.1 지식 베이스 400
* 11.1.2 정보 추출 403
### 11.2 정규 패턴 404
* 11.2.1 정규 표현식 405
* 11.2.2 기계 학습 특징 추출로서의 정보 추출 406
### 11.3 추출할 만한 정보 408
* 11.3.1 GPS 좌표 추출 408
* 11.3.2 날짜 추출 409
### 11.4 관계의 추출 415
* 11.4.1 품사 태깅 416
* 11.4.2 개체명 정규화 420
* 11.4.3 관계의 정규화와 추출 422
* 11.4.4 단어 패턴 422
* 11.4.5 분할 423
* 11.4.6 split(‘.!?’)만으로는 안 되는 이유 424
* 11.4.7 정규 표현식을 이용한 문장 분할 426
### 11.5 실제 용도 428


## CHAPTER 12 챗봇(대화 엔진) 만들기 431
### 12.1 대화 능력 432
* 12.1.1 현대적 접근 방식들 434
* 12.1.2 혼합형 접근 방식 441
### 12.2 패턴 부합 접근 방식 441
* 12.2.1 AIML을 이용한 패턴 부합 챗봇 구현 443
* 12.2.2 패턴 부합의 그래프 시각화 450
### 12.3 근거화 451
### 12.4 정보 검색 454
* 12.4.1 문맥 관리의 어려움 454
* 12.4.2 정보 검색 기반 챗봇 예제 456
* 12.4.3 Chatterbot 소개 460
### 12.5 생성 모형 463
* 12.5.1 NLPIA에 관한 대화 464
* 12.5.2 각 접근 방식의 장단점 466
### 12.6 사륜구동 467
* 12.6.1 챗봇 프레임워크 Will 468
### 12.7 설계 과정 469
### 12.8 요령과 편법 473
* 12.8.1 예측 가능한 답이 나올 질문을 던진다 473
* 12.8.2 동문서답 474
* 12.8.3 최후의 대비책은 검색 474
* 12.8.4 흥미 유지 475
* 12.8.5 인연 만들기 475
* 12.8.6 감정 담기 475
### 12.9 실제 응용 분야 476


## CHAPTER 13 규모 확장: 최적화, 병렬화, 일괄 처리 479
### 13.1 자료가 너무 많으면 480
### 13.2 NLP 알고리즘의 최적화 480
* 13.2.1 색인화 481
* 13.2.2 고급 색인화 483
* 13.2.3 Annoy를 이용한 고급 색인화 485
* 13.2.4 근사적 색인이 꼭 필요한가? 490
* 13.2.5 실숫값의 색인화: 이산화 491
### 13.3 상수 RAM 알고리즘 492
* 13.3.1 gensim 492
* 13.3.2 그래프 계산 493
### 13.4 NLP 계산 병렬화 494
* 13.4.1 GPU를 이용한 NLP 모형의 훈련 495
* 13.4.2 대여와 구매 496
* 13.4.3 GPU 대여 옵션들 497
* 13.4.4 TPU(텐서 처리 장치) 498
### 13.5 모형 훈련의 메모리 요구량 줄이기 498
### 13.6 TensorBoard를 이용한 모형 성능 평가 501
* 13.6.1 단어 내장 시각화 502


## APPENDIX A NLP 도구들 507
### A.1 Anaconda3 설치 508
### A.2 NLPIA 설치 509
### A.3 IDE 509
### A.4 우분투 패키지 관리자 510
### A.5 맥 OS 511
* A.5.1 Homebrew 511
* A.5.2 기타 개발용 도구 설치 512
* A.5.3 조율 512
### A.6 Windows 514
### A.6.1 VM 설정 515
### A.7 NLPIA의 편의 기능 515


## APPENDIX B 파이썬 즐기기와 정규 표현식 517
### B.1 문자열 다루기 518
* B.1.1 문자열 형식들: str과 bytes 518
* B.1.2 파이썬 문자열 템플릿 519
### B.2 파이썬의 매핑 자료 구조: dict와 OrderedDict 519
### B.3 정규 표현식 520
* B.3.1 |-OR 기호 520
* B.3.2 ()-그룹 묶기 521
* B.3.3 []-문자 부류 522
### B.4 코딩 스타일 523
### B.5 실력 쌓기 523


## APPENDIX C 벡터와 행렬: 기초 선형대수 524
### C.1 벡터 524
### C.1.1 거리 526


## APPENDIX D 기계 학습의 도구와 기법 531
### D.1 자료 선택과 편향 531
### D.2 얼마나 적합해야 적합된 것인가? 533
### D.3 문제를 알면 반은 해결된 것이다 534
### D.4 교차 검증 535
### D.5 과대적합 방지 536
* D.5.1 정칙화 537
* D.5.2 드롭아웃 538
* D.5.3 배치 정규화 539
### D.6 불균형 훈련 집합 539
* D.6.1 과다표집 540
* D.6.2 과소표집 540
* D.6.3 자료 증강 541
### D.7 성능 측정 542
* D.7.1 분류 모형의 성능 측정 542
* D.7.2 회귀 모형의 성능 측정 545
### D.8 전문가의 조언 545


## APPENDIX E AWS GPU 설정 548
### E.1 AWS 인스턴스 설정 549
### E.1.1 비용 관리 561


## APPENDIX F 지역 민감 해싱(LSH) 564
### F.1 고차원 벡터는 어렵다 564
* F.1.1 벡터 공간의 색인과 해시 565
* F.1.2 고차원적 사고 566
### F.2 고차원 색인화 570
* F.2.1 지역 민감 해싱 570
* F.2.2 근사 최근접 이웃 검색 571
### F.3 ‘좋아요’ 예측 571 