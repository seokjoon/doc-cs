# 00장: 강화학습과 딥러닝 개요
## 0.1 강화학습의 개요
* 0.1.1 기계 학습이란?
* 0.1.2 강화학습의 성과
* 0.1.3 강화학습의 기본 용어
* 0.1.4 탐험(Exploration)과 이용(Exploitation)
## 0.2 인공 신경망 이론
* 0.2.1 인공 신경망(Artificial Neural Network, ANN)
* 0.2.2 인공 신경망의 학습
* 0.2.3 데이터 세트의 구성
* 0.2.4 오버피팅(Overfitting)과 언더피팅(Underfitting)
* 0.2.5 텐서플로(TensorFlow)
* 0.2.6 텐서보드(TensorBoard)
## 0.3 인공 신경망 코드
* 0.3.1 분류와 회귀
* 0.3.2 회귀 문제(Boston Housing)
## 0.4 합성곱 신경망 이론
* 0.4.1 합성곱(Convolution)
* 0.4.2 합성곱 연산과 관련된 주요 용어
* 0.4.3 합성곱 신경망의 구조
## 0.5 합성곱 신경망 코드
* 0.5.1 분류 문제(MNIST 데이터 세트)


# 01장: 유니티와 ML-Agents의 개요
## 1.1 유니티 설치 및 기초
* 1.1.1 유니티 시작하기
* 1.1.2 유니티 인터페이스
## 1.2 유니티 ML-Agents
* 1.2.1 에이전트(Agent)
* 1.2.2 브레인(Brain)
* 1.2.3 아카데미(Academy)


# ▣ 02장: ML-Agents 맛보기
## 2.1 ML-Agents 프로젝트 설정
* 2.1.1 유니티 ML-Agents 내려받기
* 2.1.2 새로운 프로젝트 생성
* 2.1.3 ML-Agents 폴더를 유니티로 복사
* 2.1.4 프로젝트 설정
## 2.2 유니티 예제 열기
* 2.2.1 3DBall 환경 열기
* 2.2.2 브레인 변경
* 2.2.3 3DBall 환경 빌드
## 2.3 mlagents 설치 및 학습
* 2.3.1 mlagents 설치 및 경로 설정
* 2.3.2 Getting-started.ipynb 실행 및 코드 설명
* 2.3.3 mlagents를 이용한 학습
* 2.3.4 학습된 결과 테스트


# ▣ 03장: 소코반 환경 만들기
## 3.1 그리드월드와 소코반
## 3.2 소코반 폴더 생성 및 파일명 변경
## 3.3 그리드월드와 소코반 비교
## 3.4 박스 프리팹 제작
## 3.5 Academy 스크립트 수정
* 3.5.1 InitializeAcademy() 함수
* 3.5.2 SetEnvironment() 함수
* 3.5.3 AcademyReset() 함수
## 3.6 Agent 코드 수정
## 3.7 소코반 환경 플레이
## 3.8 소코반 환경 빌드


# 04장: Deep Q Network
## 4.1 DQN 알고리즘의 개요
## 4.2 DQN 알고리즘의 기법
* 4.2.1 경험 리플레이(Experience Replay)
* 4.2.2 타겟 네트워크(Target Network)
## 4.3 DQN 학습
## 4.4 DQN 코드
* 4.4.1 라이브러리 불러오기 및 파라미터 값 설정
* 4.4.2 Model 클래스
* 4.4.3 Agent 클래스
* 4.4.4 Main 함수
* 4.4.5 텐서보드를 이용한 성능 확인


# 05장: 드론 환경 만들기
## 5.1 드론 환경
## 5.2 드론 폴더 생성 및 파일명 변경
## 5.3 에셋 스토어에서 드론 에셋 내려받기
## 5.4 드론 에셋 환경 설정 및 플레이
## 5.5 드론 환경 제작하기
## 5.6 Agent 코드 작성
* 5.6.1 전역변수 설정
* 5.6.2 InitializeAgent 함수
* 5.6.3 CollectObservations 함수
* 5.6.4 AgentAction 함수(행동 설정)
* 5.6.5 AgentAction 함수(보상 설정)
* 5.6.6 AgentReset 함수
## 5.7 플레이어 브레인 설정 및 드론 환경 플레이
## 5.8 러닝 브레인 설정 및 드론 환경 빌드


# 06장: Deep Deterministic Policy Gradient
## 6.1 DDPG 알고리즘의 개요
## 6.2 DDPG 알고리즘의 기법들
* 6.2.1 경험 리플레이(Experience Replay)
* 6.2.2 타겟 네트워크(Target Network)
* 6.2.3 소프트 타겟 업데이트(Soft Target Update)
* 6.2.4 OU 노이즈(Ornstien Uhlenbeck Noise)
## 6.3 DDPG 학습
* 6.3.1 크리틱 네트워크 업데이트
* 6.3.2 액터 네트워크 업데이트
## 6.4 DDPG 코드
* 6.4.1 라이브러리 불러오기 및 파라미터 값 설정
* 6.4.2 OU 노이즈 클래스
* 6.4.3 Actor & Critic 클래스
* 6.4.4 Agent 클래스
* 6.4.5 Main 함수
* 6.4.6 텐서보드를 이용한 성능 확인


# 07장: 퐁 환경 만들기
## 7.1 퐁 환경 설명
## 7.2 퐁 환경 폴더 생성 및 파일명 변경
## 7.3 퐁 환경 오브젝트 만들기
* 7.3.1 게임판 만들기
* 7.3.2 골대 만들기
* 7.3.3 에이전트 만들기
## 7.4 오브젝트에 마찰 및 탄성력 적용
## 7.5 스크립트 제작
* 7.5.1 PongAgent 스크립트
* 7.5.2 PongGoalDetection 스크립트
* 7.5.3 PongAcademy 스크립트
## 7.6 브레인 추가하기
* 7.6.1 다수의 브레인을 이용한 멀티에이전트 환경 설정
* 7.6.2 하나의 브레인을 이용한 멀티에이전트 환경 설정
## 7.7 퐁 환경 빌드


# 08장: 적대적인 DQN 에이전트 만들기
## 8.1 적대적인 DQN 알고리즘의 개요
## 8.2 적대적인 DQN 알고리즘 코드
* 8.2.1 라이브러리 불러오기 및 파라미터 값 설정
* 8.2.2 Model 클래스
* 8.2.3 Agent 클래스
* 8.2.4 Main 함수
## 8.3 하나의 브레인을 이용한 적대적인 DQN 알고리즘 코드
## 8.4 적대적인 DQN 학습 결과


# 09장: 소코반 커리큘럼 학습
## 9.1 커리큘럼 학습이란?
## 9.2 커리큘럼 학습을 위한 소코반 환경
* 9.2.1 소코반 커리큘럼의 환경 설정 변경
* 9.2.2 Academy 코드 수정
* 9.2.3 Agent 코드 수정
## 9.3 DDDQN 알고리즘 이론
* 9.3.1 Double DQN
* 9.3.2 Dueling DQN
## 9.4 DDDQN의 학습 코드 작성
* 9.4.1 라이브러리 불러오기
* 9.4.2 파라미터 설정
* 9.4.3 DDDQN_Model 클래스
* 9.4.4 DDDQN_Agent 클래스
* 9.4.5 Main 함수
## 9.5 소코반 커리큘럼 학습 실행 및 결과


# 10장: 닷지 환경 만들기
## 10.1 닷지 환경 소개
## 10.2 닷지 폴더 생성 및 파일명 변경
## 10.3 씬 내부의 오브젝트 설정
* 10.3.1 아카데미 스크립트 교체
* 10.3.2 닷지 게임 환경 만들기
* 10.3.3 닷지 에이전트 만들기
* 10.3.4 공 만들기
## 10.4 스크립트 구성하기
* 10.4.1 DodgeAcademy 스크립트
* 10.4.2 DodgeAgent 스크립트
* 10.4.3 BallScript 스크립트
## 10.5 브레인 추가
## 10.6 환경 빌드
## 10.7 사람의 플레이 데이터 저장하기
* 10.7.1 Demonstration 추가
* 10.7.2 게임 실행 및 학습 데이터 녹화
* 10.7.3 녹화 데이터 저장 경로


# 11장: Behavioral Cloning
## 11.1 Behavioral Cloning 알고리즘의 개요
## 11.2 Behavioral Cloning 알고리즘의 기법
* 11.2.1 보상이 음수인 데이터 제외하기
* 11.2.2 드롭아웃(Dropout)
## 11.3 Behavioral Cloning 학습
## 11.4 Behavioral Cloning 코드
* 11.4.1 라이브러리 불러오기
* 11.4.2 파라미터 값 설정
* 11.4.3 Model 클래스
* 11.4.4 Agent 클래스
* 11.4.5 Main 함수
## 11.5 ml-agents의 내장 Behavioral Cloning


# 12장: 마무리
## 12.2 여러 개의 ML-Agents 환경 동시에 실행하기