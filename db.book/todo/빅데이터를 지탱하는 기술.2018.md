# CHAPTER 1 빅데이터의 기초 지식 _ 1
## 1-1 [배경] 빅데이터의 정착 3
* 분산 시스템에 의한 데이터 처리의 고속화 ― 빅데이터의 취급하기 어려운 점을 극복한 두 가지 대표 기술 3
* 분산 시스템의 비즈니스 이용 개척 ― 데이터 웨어하우스와의 공존 7
* 직접 할 수 있는 데이터 분석 폭 확대 ― 클라우드 서비스와 데이터 디스커버리로 가속하는 빅데이터의 활용 8
## 1-2 빅데이터 시대의 데이터 분석 기반 11
* [재입문] 빅데이터의 기술 ― 분산 시스템을 활용해서 데이터를 가공해 나가는 구조 11
* 데이터 웨어하우스와 데이터 마트 ― 데이터 파이프라인 기본형 16
* 데이터 레이크 ― 데이터를 그대로 축적 17
* 데이터 분석 기반을 단계적으로 발전시키기 ― 팀과 역할 분담, 스몰 스타트와 확장 19
* 데이터를 수집하는 목적 ― ‘검색’, ‘가공’, ‘시각화’의 세 가지 예 22
* 확증적 데이터 분석과 탐색적 데이터 분석 25
## 1-3 [속성 학습] 스크립트 언어에 의한 특별 분석과 데이터 프레임 26
* 데이터 처리와 스크립트 언어 ― 인기 언어인 파이썬과 데이터 프레임 26
* 데이터 프레임, 기초 중의 기초 ― ‘배열 안의 배열’로부터 작성 27
* 웹 서버의 액세스 로그의 예 ― pandas의 데이터 프레임으로 간단히 처리 28
* 시계열 데이터를 대화식으로 집계하기 ― 데이터 프레임을 그대로 사용한 데이터 집계 30
* SQL의 결과를 데이터 프레임으로 활용하기 31
## 1-4 BI 도구와 모니터링 33
* 스프레드시트에 의한 모니터링 ― 프로젝트의 현재 상황 파악하기 33
* 데이터에 근거한 의사 결정 ― KPI 모니터링 35
* 변화를 파악하고 세부 사항을 이해하기 ― BI 도구의 활용 37
* 수작업과 자동화해야 할 것의 경계를 판별하기 39


# CHAPTER 2 빅데이터의 탐색 _ 43
## 2-1 크로스 집계의 기본 45
* 트랜잭션 테이블, 크로스 테이블, 피벗 테이블 ― ‘크로스 집계’의 개념 45
* 룩업 테이블 ― 테이블을 결합하여 속성 늘리기 47
* SQL에 의한 테이블의 집계 ― 대량 데이터의 크로스 집계 사전 준비 50
* 데이터 집계 ? 데이터 마트 ? 시각화 ― 시스템 구성은 데이터 마트의 크기에 따라 결정된다 55
## 2-2 열 지향 스토리지에 의한 고속화 56
* 데이터베이스의 지연을 줄이기 56
* 열 지향 데이터베이스 접근 ― 칼럼을 압축하여 디스크 I/O를 줄이기 58
* MPP 데이터베이스의 접근 방식 ― 병렬화에 의해 멀티 코어 활용하기 61
## 2-3 애드 혹 분석과 시각화 도구 64
* Jupyter Notebook에 의한 애드 혹 분석 ― 노트북에 분석 과정 기록하기 64
* 대시보드 도구 ― 정기적으로 집계 결과를 시각화하기 68
* BI 도구 ― 대화적인 대시보드 75
## 2-4 데이터 마트의 기본 구조 77
* 시각화에 적합한 데이터 마트 만들기 ― OLAP 77
* 테이블을 비정규화하기 79
* 다차원 모델 시각화에 대비하여 테이블을 추상화하기 82


# CHAPTER 3 빅데이터의 분산 처리 _ 87
## 3-1 대규모 분산 처리의 프레임워크 89
* 구조화 데이터와 비구조화 데이터 89
* Hadoop ― 분산 데이터 처리의 공통 플랫폼 92
* Spark ― 인 메모리 형의 고속 데이터 처리 99
## 3-2 쿼리 엔진 101
* 데이터 마트 구축의 파이프라인 101
* Hive에 의한 구조화 데이터 작성 102
* 대화형 쿼리 엔진 Presto의 구조 ― Presto로 구조화 데이터 집계하기 109
* 데이터 분석의 프레임워크 선택하기 ― MPP 데이터베이스, Hive, Presto, Spark 115
## 3-3 데이터 마트의 구축 119
* 팩트 테이블 ― 시계열 데이터 축적하기 119
* 집계 테이블 ― 레코드 수 줄이기 122
* 스냅샷 테이블 ― 마스터의 상태를 기록하기 123
* 이력 테이블 ― 마스터 변화 기록하기 127
* [마지막 단계] 디멘전을 추가하여 비정규화 테이블 완성시키기 127


# CHAPTER 4 빅데이터의 축적 _ 131
## 4-1 벌크 형과 스트리밍 형의 데이터 수집 133
* 객체 스토리지와 데이터 수집 ― 분산 스토리지에 데이터 읽어들이기 133
* 벌크 형의 데이터 전송 ― ETL 서버의 설치 필요성 135
* 스트리밍 형의 데이터 전송 ― 계속해서 전송되어 오는 작은 데이터를 취급하기 위한 데이터 전송 137
## 4-2 [성능×신뢰성] 메시지 배송의 트레이드 오프 143
* 메시지 브로커 ― 스토리지의 성능 문제를 해결하는 중간층의 설치 143
* 메시지 배송을 확실하게 실시하는 것은 어렵다 ― 신뢰성 문제와 세 가지 설계 방식 146
* 중복 제거는 높은 비용의 오퍼레이션 149
* 데이터 수집의 파이프라인 ― 장기적인 데이터 분석에 적합한 스토리지 152
## 4-3 시계열 데이터의 최적화 154
* 프로세스 시간와 이벤트 시간 ― 데이터 분석의 대상은 주로 이벤트 시간 154
* 프로세스 시간에 의한 분할과 문제점 ― 최대한 피하고 싶은 풀 스캔 154
* 시계열 인덱스 ― 이벤트 시간에 의한 집계의 효율화 ① 156
* 조건절 푸쉬다운 ― 이벤트 시간에 의한 집계의 효율화 ② 157
* 이벤트 시간에 의한 분할 ― 테이블 파티셔닝, 시계열 테이블 158
## 4-4 비구조화 데이터의 분산 스토리지 161
* [기본 전략] NoSQL 데이터베이스에 의한 데이터 활용 161
* 분산 KVS ― 디스크로의 쓰기 성능을 높이기 162
* 와이드 칼럼 스토어 ― 구조화 데이터를 분석해서 저장하기 166
* 도큐먼트 스토어 ― 스키마리스 데이터 관리하기 169
* 검색 엔진 ― 키워드 검색으로 데이터 검색 171
 

# CHAPTER 5 빅데이터의 파이프라인 _ 177
## 5-1 워크플로 관리 179
* [기초 지식] 워크플로 관리 ― 데이터의 흐름을 일원 관리하기 179
* 오류로부터의 복구 방법 먼저 생각하기 183
* 멱등한 조작으로 태스크를 기술하기 ― 동일 태스크를 여러 번 실행해도 동일한 결과가 된다 188
* 워크플로 전체를 멱등으로 하기 194
* 태스크 큐 ― 자원의 소비량 컨트롤하기 195
## 5-2 배치 형의 데이터 플로우 199
* MapReduce의 시대는 끝났다 ― 데이터 플로우와 워크플로 199
* MapReduce를 대신할 새로운 프레임워크 ― DAG에 의한 내부 표현 201
* 데이터 플로우와 워크플로를 조합하기 204
* 데이터 플로우와 SQL을 나누어 사용하기 ― 데이터 웨어하우스의 파이프라인과 데이터 마트의 파이프라인 207
## 5-3 스트리밍 형의 데이터 플로우 209
* 배치 처리와 스트림 처리로 경로 나누기 209
* 배치 처리와 스트림 처리 통합하기 211
* 스트림 처리의 결과를 배치 처리로 치환하기 ― 스트림 처리의 두 가지 문제에 대한 대처 214
* 아웃 오브 오더의 데이터 처리 217


# CHAPTER 6 빅데이터 분석 기반의 구축 _ 223
## 6-1 스키마리스 데이터의 애드 혹 분석 225
* 스키마리스 데이터 수집하기 225
* 대화식 실행 환경의 준비 228
* Spark에 의한 분산 환경 ― 데이터양이 늘어도 대응 가능하게 하기 232
* 데이터를 집계해서 데이터 마트 구축하기 237
* BI 도구로 데이터 시각화하기 241
## 6-2 Hadoop에 의한 데이터 파이프라인 245
* 일일 배치 처리를 태스크화하기 245
* [태스크 1] Embulk에 의한 데이터 추출 246
* [태스크 2] Hive에 의한 데이터 구조화 248
* [태스크 3] Presto에 의한 데이터 집계 250
## 6-3 워크플로 관리 도구에 의한 자동화 253
* Airflow ― 스크립트 형의 워크플로 관리 253
* 워크플로를 터미널로부터 실행하기 257
* 스케줄러를 기동하여 DAG를 정기 실행하기 260
* 태스크가 소비하는 자원 제어하기 265
* Hadoop의 데이터 파이프라인을 실행하기 266
## 6-4 클라우드 서비스에 의한 데이터 파이프라인 268
* 데이터 분석과 클라우드 서비스의 관계 268
* 아마존 웹 서비스 270
* 구글 클라우드 플랫폼 272
* 트레주어 데이터 274